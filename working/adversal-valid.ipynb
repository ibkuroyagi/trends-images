{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":["file_No = 30\n","fold = 0"],"execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Importing dependencies"]},{"metadata":{"trusted":true},"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset\n","from sklearn.metrics import f1_score\n","import numpy as np\n","import pandas as pd\n","\n","import random\n","from tqdm.notebook import tqdm\n","import math\n","from functools import partial\n","import h5py\n","from datetime import datetime\n","import os\n","import time\n","import gc\n","from joblib import Parallel, delayed\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from sklearn.decomposition import PCA\n","import nilearn as nl\n","import nilearn.plotting as nlplt\n","import nibabel as nib"],"execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Config"]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":["class config:\n","    epochs = 100\n","    batch_size = 16\n","    test_batch_size = 16\n","    learning_rate = 1e-3\n","    fMRI_mask_path = '../input/trends-assessment-prediction/fMRI_mask.nii'\n","    root_train_path = '../input/trends-assessment-prediction/fMRI_train'\n","    root_test_path = '../input/trends-assessment-prediction/fMRI_test'\n","    num_folds = 5\n","    seed = 2020\n","    verbose = True\n","    verbose_step = 1\n","    num_workers = 4\n","    test_num_workers = 4\n","    target = [\"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]\n","    weight = [0.3, 0.175, 0.175, 0.175, 0.175]\n","    # cross validationをするときはここでfoldを変更する\n","    fold = fold"],"execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"tags":[]},"cell_type":"code","source":["print(\"fold\",config.fold, \"file_No\", file_No)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":"fold 0 file_No 30\n"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","seed_everything(config.seed)"],"execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Metrics"]},{"metadata":{"trusted":true},"cell_type":"code","source":["class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","    \n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","    \n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Loss function"]},{"metadata":{"trusted":true,"tags":[]},"cell_type":"code","source":["def focal_loss(labels, logits, alpha, gamma):\n","    \"\"\"Compute the focal loss between `logits` and the ground truth `labels`.\n","    Focal loss = -alpha_t * (1-pt)^gamma * log(pt)\n","    where pt is the probability of being classified to the true class.\n","    pt = p (if true class), otherwise pt = 1 - p. p = sigmoid(logit).\n","    Args:\n","      labels: A float tensor of size [batch, num_classes].\n","      logits: A float tensor of size [batch, num_classes].\n","      alpha: A float tensor of size [batch_size]\n","        specifying per-example weight for balanced cross entropy.\n","      gamma: A float scalar modulating loss from hard and easy examples.\n","    Returns:\n","      focal_loss: A float32 scalar representing normalized total loss.\n","    \"\"\"    \n","    BCLoss = F.binary_cross_entropy_with_logits(input = logits, target = labels,reduction = \"none\")\n","\n","    if gamma == 0.0:\n","        modulator = 1.0\n","    else:\n","        modulator = torch.exp(-gamma * labels * logits - gamma * torch.log(1 + \n","            torch.exp(-1.0 * logits)))\n","\n","    loss = modulator * BCLoss\n","\n","    weighted_loss = alpha * loss\n","    focal_loss = torch.sum(weighted_loss)\n","\n","    focal_loss /= torch.sum(labels)\n","    return focal_loss\n","\n","\n","\n","def CB_loss(labels, logits, samples_per_cls, no_of_classes, loss_type, beta, gamma):\n","    \"\"\"Compute the Class Balanced Loss between `logits` and the ground truth `labels`.\n","    Class Balanced Loss: ((1-beta)/(1-beta^n))*Loss(labels, logits)\n","    where Loss is one of the standard losses used for Neural Networks.\n","    Args:\n","      labels: A int tensor of size [batch].\n","      logits: A float tensor of size [batch, no_of_classes].\n","      samples_per_cls: A python list of size [no_of_classes].\n","      no_of_classes: total number of classes. int\n","      loss_type: string. One of \"sigmoid\", \"focal\", \"softmax\".\n","      beta: float. Hyperparameter for Class balanced loss.\n","      gamma: float. Hyperparameter for Focal loss.\n","    Returns:\n","      cb_loss: A float tensor representing class balanced loss\n","    \"\"\"\n","    effective_num = 1.0 - np.power(beta, samples_per_cls)\n","    weights = (1.0 - beta) / np.array(effective_num)\n","    weights = weights / np.sum(weights) * no_of_classes\n","\n","    labels_one_hot = F.one_hot(labels, no_of_classes).float()\n","\n","    weights = torch.tensor(weights).float().to(\"cpu\")\n","    weights = weights.unsqueeze(0)\n","    weights = weights.repeat(labels_one_hot.shape[0],1) * labels_one_hot\n","    weights = weights.sum(1)\n","    weights = weights.unsqueeze(1)\n","    weights = weights.repeat(1,no_of_classes)\n","\n","    if loss_type == \"focal\":\n","        cb_loss = focal_loss(labels_one_hot, logits, weights, gamma)\n","    elif loss_type == \"sigmoid\":\n","        cb_loss = F.binary_cross_entropy_with_logits(input = logits,target = labels_one_hot, weight = weights)\n","    elif loss_type == \"softmax\":\n","        pred = logits.softmax(dim = 1)\n","        cb_loss = F.binary_cross_entropy(input = pred, target = labels_one_hot, weight = weights)\n","    return cb_loss\n","no_of_classes = 2\n","logits = F.softmax(torch.rand(10,no_of_classes)).float()#.view(-1)\n","labels = torch.empty(10).random_(2).to(torch.int64)\n","print(logits)\n","print(labels)\n","beta = 0.9999\n","gamma = 1.0\n","samples_per_cls = [400, 4000]\n","loss_type = \"focal\"\n","cb_loss = CB_loss(labels, logits, samples_per_cls, no_of_classes,loss_type, beta, gamma)\n","print(cb_loss)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":"tensor([[0.5943, 0.4057],\n        [0.6159, 0.3841],\n        [0.5529, 0.4471],\n        [0.4451, 0.5549],\n        [0.4062, 0.5938],\n        [0.4346, 0.5654],\n        [0.5699, 0.4301],\n        [0.5822, 0.4178],\n        [0.6635, 0.3365],\n        [0.3180, 0.6820]])\ntensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0])\ntensor(1.1445)\n"}]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"tensor(1.1445)"},"metadata":{},"execution_count":8}],"source":["c = CB_loss\n","c(labels, logits, samples_per_cls, no_of_classes,loss_type, beta, gamma)"]},{"metadata":{},"cell_type":"markdown","source":["# Model"]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":["__all__ = [\n","    'resnet10', \n","    'resnet18', \n","    'resnet34', \n","    'resnet50', \n","    'resnet101',\n","    'resnet152', \n","    'resnet200'\n","]\n","\n","def conv3x3x3(in_planes, out_planes, stride=1, dilation=1):\n","    # 3x3x3 convolution with padding\n","    return nn.Conv3d(\n","        in_planes,\n","        out_planes,\n","        kernel_size=3,\n","        dilation=dilation,\n","        stride=stride,\n","        padding=dilation,\n","        bias=False)\n","\n","def downsample_basic_block(x, planes, stride, no_cuda=False):\n","    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n","    zero_pads = torch.Tensor(out.size(0), planes - out.size(1), out.size(2), out.size(3), out.size(4)).zero_()\n","    if not no_cuda:\n","        if isinstance(out.data, torch.cuda.FloatTensor):\n","            zero_pads = zero_pads.cuda()\n","\n","    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n","    return out\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3x3(inplanes, planes, stride=stride, dilation=dilation)\n","        self.bn1 = nn.BatchNorm3d(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3x3(planes, planes, dilation=dilation)\n","        self.bn2 = nn.BatchNorm3d(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.dilation = dilation\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm3d(planes)\n","        self.conv2 = nn.Conv3d(\n","            planes, planes, kernel_size=3, stride=stride, dilation=dilation, padding=dilation, bias=False)\n","        self.bn2 = nn.BatchNorm3d(planes)\n","        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm3d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.dilation = dilation\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","        return out\n","\n","class ResNet3D(nn.Module):\n","\n","    def __init__(self,\n","                 block,\n","                 layers,\n","                 shortcut_type='B',\n","                 num_class = 5,\n","                 no_cuda=False):\n","\n","        self.inplanes = 64\n","        self.no_cuda = no_cuda\n","        super(ResNet3D, self).__init__()\n","\n","        # 3D conv net\n","        self.conv1 = nn.Conv3d(53, 64, kernel_size=7, stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n","        # self.conv1 = nn.Conv3d(1, 64, kernel_size=7, stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n","        self.bn1 = nn.BatchNorm3d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)\n","        self.layer2 = self._make_layer(\n","            block, 64*2, layers[1], shortcut_type, stride=2)\n","        self.layer3 = self._make_layer(\n","            block, 128*2, layers[2], shortcut_type, stride=1, dilation=2)\n","        self.layer4 = self._make_layer(\n","            block, 256*2, layers[3], shortcut_type, stride=1, dilation=4)\n","\n","        self.fea_dim = 256*2 * block.expansion\n","        self.fc = nn.Sequential(nn.Linear(self.fea_dim, num_class, bias=True))\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv3d):\n","                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')\n","            elif isinstance(m, nn.BatchNorm3d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1, dilation=1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","\n","            if shortcut_type == 'A':\n","                downsample = partial(\n","                    downsample_basic_block,\n","                    planes=planes * block.expansion,\n","                    stride=stride,\n","                    no_cuda=self.no_cuda)\n","            else:\n","                downsample = nn.Sequential(\n","                    nn.Conv3d(\n","                        self.inplanes,\n","                        planes * block.expansion,\n","                        kernel_size=1,\n","                        stride=stride,\n","                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride=stride, dilation=dilation, downsample=downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, dilation=dilation))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.conv1( x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = F.adaptive_avg_pool3d(x, (1, 1, 1))\n","        emb_3d = x.view((-1, self.fea_dim))\n","        out = self.fc(emb_3d)\n","        return out\n","\n","\n","def resnet10(**kwargs):\n","    \"\"\"Constructs a ResNet-18 model.\n","    \"\"\"\n","    model = ResNet3D(BasicBlock, [1, 1, 1, 1],**kwargs)\n","    return model\n","\n","def resnet3d_10(**kwargs):\n","    \"\"\"Constructs a ResNet-18 model.\n","    \"\"\"\n","    model = ResNet3D(BasicBlock, [1, 1, 1, 1], **kwargs)\n","    return model\n","\n","def resnet18(**kwargs):\n","    \"\"\"Constructs a ResNet-18 model.\n","    \"\"\"\n","    model = ResNet3D(BasicBlock, [2, 2, 2, 2], **kwargs)\n","    return model\n","\n","def resnet34(**kwargs):\n","    \"\"\"Constructs a ResNet-34 model.\n","    \"\"\"\n","    model = ResNet3D(BasicBlock, [3, 4, 6, 3], **kwargs)\n","    return model\n","\n","def resnet50(**kwargs):\n","    \"\"\"Constructs a ResNet-50 model.\n","    \"\"\"\n","    model = ResNet3D(Bottleneck, [3, 4, 6, 3], **kwargs)\n","    return model\n","\n","def resnet101(**kwargs):\n","    \"\"\"Constructs a ResNet-101 model.\n","    \"\"\"\n","    model = ResNet3D(Bottleneck, [3, 4, 23, 3], **kwargs)\n","    return model\n","\n","def resnet152(**kwargs):\n","    \"\"\"Constructs a ResNet-101 model.\n","    \"\"\"\n","    model = ResNet3D(Bottleneck, [3, 8, 36, 3], **kwargs)\n","    return model\n","\n","def resnet200(**kwargs):\n","    \"\"\"Constructs a ResNet-101 model.\n","    \"\"\"\n","    model = ResNet3D(Bottleneck, [3, 24, 36, 3], **kwargs)\n","    return model\n"],"execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["class TReNDSModel(nn.Module):\n","    def __init__(self):\n","        super(TReNDSModel, self).__init__()\n","        \n","        # modules = list(resnet50().children())[:-1]\n","        modules = list(resnet10().children())[:-1]\n","        self.resnet = nn.Sequential(*modules)\n","        \n","        self.m1 = nn.MaxPool3d(kernel_size=(3, 3, 3))\n","        self.f0 = nn.Flatten()\n","        self.l0 = nn.Linear(5500, 1024)\n","        # self.l0 = nn.Linear(17788, 1024) # resnet10 -> 4096, resnet50 -> 16384\n","        self.p0 = nn.PReLU()\n","        self.l1 = nn.Linear(1024, 256)\n","        self.p1 = nn.PReLU()\n","        self.l2 = nn.Linear(256, 2)\n","        self.sigmoid = nn.Sigmoid()\n","        \n","        \n","    def forward(self, inputs, fnc, loading):\n","        features = self.resnet(inputs)\n","        x = self.m1(features)\n","        flatten = self.f0(x) #shape=(batch, 16384) +(batch, 1378)) + (bathc, 26)\n","        x = torch.cat([flatten, fnc, loading], dim=1) #shape(batch, 16384+1378+26)\n","        x = self.l0(x)\n","        x = self.p0(x)\n","        x = self.l1(x)\n","        x = self.p1(x)\n","        out = self.l2(x)\n","        out = self.sigmoid(out)\n","        return out"],"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"tags":[]},"outputs":[],"source":["# m = nn.Sigmoid()\n","# loss = nn.BCELoss()\n","# input = torch.randn(3, requires_grad=True)\n","# target = torch.empty(3).random_(2)\n","# print(input.shape, target.shape)\n","# print(input, target)\n","# output = loss(m(input), target)\n","# print(output)\n","# output.backward()\n","# print(output)"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# def count_parameters(model):\n","#     params = []\n","#     for p in model.parameters():\n","#         params.append(p.numel()) \n","#     return params\n","\n","# def count_trainable_parameters(model):\n","#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","\n","# model = TReNDSModel()\n","# model\n","\n","\n","# num_parameters=count_parameters(model)\n","# print(num_parameters)\n","# num_parameters=count_trainable_parameters(model)\n","# print(num_parameters)"],"execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Dataset"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# class MRIMapDataset(Dataset):\n","#     def __init__(self, df=None, fnc=None, loading=None, mode=\"train\"):\n","#         super(Dataset, self).__init__()\n","#         self.mode = mode\n","#         self.fnc = fnc.iloc[:, 1:-2].values\n","#         self.loading = loading.iloc[:, 1:-2].values\n","        \n","#         if mode == \"train\":\n","#             # self.labels = df[['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']].values\n","#             self.labels = df[\"is_site2\"].values\n","#             self.list_IDs = df[\"Id\"].values.astype(str)\n","#         elif mode == \"test\":\n","#             list1 = os.listdir(config.root_test_path)\n","#             self.list_IDs = sorted(list1)\n","\n","#     def __len__(self):\n","#         return len(self.list_IDs)\n","    \n","#     def __getitem__(self, idx):\n","#         if self.mode == \"train\":\n","#             scan_id = self.list_IDs[idx]        \n","#             subject_filename = config.root_train_path + '/' + scan_id + '.mat'\n","#             subject_data = h5py.File(subject_filename, 'r')['SM_feature'][()]\n","#             subject_data = np.moveaxis(subject_data, [0, 1, 2, 3], [3, 2, 1, 0])\n","#             fnc = self.fnc[idx] / 600.0\n","#             loading = self.loading[idx]\n","#             return {\n","#                 'scan_maps': torch.tensor(subject_data, dtype=torch.float),\n","#                 'fnc': torch.tensor(fnc, dtype=torch.float),\n","#                 'loading': torch.tensor(loading, dtype=torch.float),\n","#                 'targets': torch.tensor(self.labels[idx, ], dtype=torch.int)\n","#             }\n","#         elif self.mode == \"test\":\n","#             scan_id = self.list_IDs[idx]        \n","#             subject_filename = config.root_test_path + '/' + scan_id\n","#             subject_data = h5py.File(subject_filename, 'r')['SM_feature'][()]\n","#             subject_data = np.moveaxis(subject_data, [0, 1, 2, 3], [3, 2, 1, 0])\n","#             fnc = self.fnc[idx].values / 600.0\n","#             loading = self.loading[idx].values\n","#             return {\n","#                 'scan_maps': torch.tensor(subject_data, dtype=torch.float),\n","#                 'fnc': torch.tensor(fnc, dtype=torch.float),\n","#                 'loading': torch.tensor(loading, dtype=torch.float),\n","#             }"],"execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["class MRIMapDataset(Dataset):\n","    def __init__(self, df=None, fnc=None, loading=None, mode=\"train\", fMRI_path=None):\n","        super(Dataset, self).__init__()\n","        self.mode = mode\n","        self.fnc = fnc.iloc[:, 1:-2].values\n","        self.loading = loading.iloc[:, 1:-2].values\n","        self.fMRI_path = fMRI_path.values\n","        if mode == \"train\":\n","            self.labels = df[\"is_site2\"].values\n","\n","    def __len__(self):\n","        return len(self.fMRI_path)\n","    \n","    def __getitem__(self, idx):\n","        if self.mode == \"train\":\n","            subject_filename = self.fMRI_path[idx]  \n","            subject_data = h5py.File(subject_filename, 'r')['SM_feature'][()]\n","            subject_data = np.moveaxis(subject_data, [0, 1, 2, 3], [3, 2, 1, 0])\n","            fnc = self.fnc[idx] / 600.0\n","            loading = self.loading[idx]\n","            return {\n","                'scan_maps': torch.tensor(subject_data, dtype=torch.float),\n","                'fnc': torch.tensor(fnc, dtype=torch.float),\n","                'loading': torch.tensor(loading, dtype=torch.float),\n","                'targets': torch.tensor(self.labels[idx, ], dtype=torch.int)\n","            }\n","        elif self.mode == \"test\":\n","            # scan_id = self.list_IDs[idx]        \n","            subject_filename = self.fMRI_path[idx]  # config.root_test_path + '/' + scan_id\n","            subject_data = h5py.File(subject_filename, 'r')['SM_feature'][()]\n","            subject_data = np.moveaxis(subject_data, [0, 1, 2, 3], [3, 2, 1, 0])\n","            fnc = self.fnc[idx].values / 600.0\n","            loading = self.loading[idx].values\n","            return {\n","                'scan_maps': torch.tensor(subject_data, dtype=torch.float),\n","                'fnc': torch.tensor(fnc, dtype=torch.float),\n","                'loading': torch.tensor(loading, dtype=torch.float),\n","            }"]},{"metadata":{},"cell_type":"markdown","source":["## Early Stopping"]},{"metadata":{"trusted":true},"cell_type":"code","source":["class EarlyStopping:\n","    def __init__(self, patience=5, checkpoint_path='checkpoint.pth', device=\"cpu\"):\n","        self.patience = patience\n","        self.checkpoint_path = checkpoint_path\n","        self.counter = 0\n","        self.best_score = None\n","        self.device = device\n","\n","    def load_best_weights(self, model):\n","        model.load_state_dict(torch.load(self.checkpoint_path, map_location=self.device))\n","\n","    def __call__(self, score, model, mode=\"min\"):\n","        # cpuでも使用できるようにするためにパラメータを一度cpuに変換してから保存し、再度deviceに直す\n","        if mode == \"max\":\n","            if self.best_score is None or (score > self.best_score):\n","                torch.save(model.to('cpu').state_dict(), self.checkpoint_path)\n","                model.to(self.device)\n","                self.best_score, self.counter = score, 0\n","                return 1\n","            else:\n","                self.counter += 1\n","                if self.counter >= self.patience:\n","                    return 2\n","        elif mode == \"min\":\n","            if self.best_score is None or (score < self.best_score):\n","                torch.save(model.to('cpu').state_dict(), self.checkpoint_path)\n","                model.to(self.device)\n","                self.best_score, self.counter = score, 0\n","                return 1\n","            else:\n","                self.counter += 1\n","                if self.counter >= self.patience:\n","                    return 2\n","        return 0\n"],"execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# GPU Fitter"]},{"metadata":{"trusted":true},"cell_type":"code","source":["class GPUFitter:\n","    def __init__(self, model, fold, device, config, save_model_path=\"checkpoint.pth\", log_path=\"log.csv\"):\n","        self.model = model\n","        self.device = device\n","        self.log_path = log_path[:-4] +f\"_fold{fold}_No{file_No}.csv\"\n","        self.save_model_path = save_model_path[:-4] +f\"_fold{fold}_No{file_No}.pth\"\n","        \n","        self.epoch = 0\n","        self.fold = fold\n","        self.config = config\n","                \n","        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.learning_rate)\n","        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","            self.optimizer, \n","            mode='min', \n","            patience=3, \n","            factor=0.3, \n","            verbose=True\n","        )\n","        self.early_stopping = EarlyStopping(patience=10, device=device, checkpoint_path=self.save_model_path)\n","        # logits = F.softmax(torch.rand(10,no_of_classes)).float()#.view(-1)\n","        # labels = torch.empty(10).random_(2).to(torch.int64)\n","        self.no_of_classes = 2\n","        self.beta = 0.9999\n","        self.gamma = 2.0\n","        self.samples_per_cls = [1176 * 4, 102 * 4]\n","        self.samples_per_cls_valid = [1176 * 4, 102 * 1]\n","        self.loss_type = \"focal\"\n","        # cb_loss = CB_loss(labels, logits, samples_per_cls, no_of_classes,loss_type, beta, gamma)\n","        self.criterion = CB_loss  # TReNDSLoss(self.device)\n","                \n","        self.log(f'Fitter prepared for fold {self.fold}. Device is {self.device} target:{config.target[target_id]}')\n","        self.columns = [\"loss\", \"score\", \"val_loss\", \"val_score\", \"lr\"]\n","        self.log_df = pd.DataFrame(columns=self.columns)\n","        \n","    def fit(self, train_loader, valid_loader):\n","        for e in range(self.config.epochs):\n","            lr = self.optimizer.param_groups[0]['lr']\n","            if self.config.verbose:\n","                timestamp = datetime.utcnow().isoformat()\n","                self.log(f'\\n{timestamp}\\nLR:{lr:.5f}')\n","            \n","            t = time.time()\n","            loss, score = self.train_one_epoch(train_loader)\n","            val_loss, val_score = self.validation_one_epoch(valid_loader)\n","            print(f'Epoch: {self.epoch}, loss: {loss.avg:.5f}, score: {score:.5f},'\\\n","                  f'val_loss: {val_loss.avg:.5f}, val_score: {val_score:.5f},'\\\n","                  f'time:{(time.time() - t):.5f}, lr{lr:.7f}' )\n","            res = self.early_stopping(val_score, self.model, mode=\"max\")\n","            self.scheduler.step(val_loss.avg)\n","            tmp = pd.DataFrame([[loss.avg, score, val_loss.avg, val_score, lr]], columns=self.columns)\n","            self.log_df = pd.concat([self.log_df, tmp], axis=0)\n","            self.log_df.to_csv(self.log_path, index=False)\n","            if res == 2:\n","                print(\"Early Stopping\")\n","                print(self.log_path)\n","                print(self.save_model_path)\n","                break\n","            self.epoch += 1\n","    \n","    def train_one_epoch(self, train_loader):\n","        self.model.train()\n","        losses = AverageMeter()\n","        t = time.time()\n","        _targets = []\n","        _outputs = []\n","        for step, data in enumerate(train_loader):\n","            scan_maps = data['scan_maps']\n","            fnc = data['fnc']\n","            loading =data['loading']\n","            targets = data['targets']\n","            \n","            scan_maps = scan_maps.to(self.device, dtype=torch.float)\n","            fnc = fnc.to(self.device, dtype=torch.float)\n","            loading = loading.to(self.device, dtype=torch.float)\n","            targets = targets.to(self.device, dtype=torch.int64)\n","            self.optimizer.zero_grad()\n","            \n","            outputs = self.model(scan_maps, fnc, loading)\n","            loss = self.criterion(targets, outputs, self.samples_per_cls, self.no_of_classes, self.loss_type, self.beta, self.gamma)\n","            \n","            batch_size = scan_maps.size(0)\n","            losses.update(loss.detach().item(), batch_size)\n","            \n","            targets = targets.detach().cpu().numpy()\n","            outputs = outputs.detach().cpu().numpy()\n","            _targets += list(targets)\n","            _outputs += list(outputs)\n","            loss.backward()\n","            self.optimizer.step()\n","            if self.config.verbose:\n","                if step % self.config.verbose_step == 0:\n","                    self.log(\n","                        f'Train Step {step}, ' + \\\n","                        f'fold {self.fold}, ' + \\\n","                        f'loss: {losses.avg:.5f}, ' + \\\n","                        f'time: {(time.time() - t):.5f}'\n","                    )\n","        scores = f1_score(_targets, _outputs) \n","        return losses, scores\n","    \n","    def validation_one_epoch(self, validation_loader):\n","        self.model.eval()\n","        \n","        losses = AverageMeter()\n","        t = time.time()\n","        _targets = []\n","        _outputs = []\n","        with torch.no_grad():\n","            for step, data in enumerate(validation_loader):\n","                scan_maps = data['scan_maps']\n","                fnc = data['fnc']\n","                loading =data['loading']\n","                targets = data['targets']\n","\n","                scan_maps = scan_maps.to(self.device, dtype=torch.float)\n","                fnc = fnc.to(self.device, dtype=torch.float)\n","                loading = loading.to(self.device, dtype=torch.float)\n","                targets = targets.to(self.device, dtype=torch.int64)\n","                outputs = self.model(scan_maps, fnc, loading)\n","\n","                loss = self.criterion(targets, outputs, self.samples_per_cls_valid, self.no_of_classes, self.loss_type, self.beta, self.gamma)\n","                batch_size = scan_maps.size(0)\n","                losses.update(loss.detach().item(), batch_size)\n","                \n","                targets = targets.detach().cpu().numpy()\n","                outputs = outputs.detach().cpu().numpy()\n","                _targets += list(targets)\n","                _outputs += list(outputs)\n","                if self.config.verbose:\n","                    if step % self.config.verbose_step == 0:\n","                        self.log(\n","                        f'Validation Step {step}, ' + \\\n","                        f'fold {self.fold}, ' + \\\n","                        f'loss: {losses.avg:.5f}, ' + \\\n","                        # f'competition metric: {scores.avg:.5f}, ' + \\\n","                        f'time: {(time.time() - t):.5f}'\n","                        )\n","        scores = f1_score(_targets, _outputs)\n","        return losses, scores\n","    \n","    def log(self, message):\n","        if self.config.verbose:\n","            print(message)"],"execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Loading data"]},{"metadata":{},"cell_type":"markdown","source":["## num_workerについて\n","\n","0だとメインプロセスのみがバッチをロードして、1以上だとサブプロセスが生えて代わりにロードしてくれるらしい  \n","これを1以上にすると、Pythonのコードを実行してるメインプロセスとは別のワーカープロセスがメインプロセスと並列的にデータのロードを行ってメモリにキューして行ってくれるので、メインプロセスは、データのロード以外の仕事に集中できる。  \n","ただし、ワーカープロセス数は増やせばいいってもんじゃなくて、メインプロセスの他の処理の忙しさとか、CPUコア数とかバッチサイズとかにも複雑に依存するので、実測値がデフォルトよりもよくなるかはわからん。  \n","ワーカープロセスが過多だと、メインプロセスが次のバッチを必要とするまでにメモリが詰まったり、その分CPUが占領され流とか、メモリが足りなくなるとか  \n","\n","#### 例\n","すべてのデータを使うと (num_worker=8, batch_size=16)のときはメモリエラー  \n","すべてのデータを使うと (num_worker=4, batch_size=16)のときはうまくいく  \n","https://deeplizard.com/learn/video/kWVgvsejXsE これがnum_workerについて分かりやすい\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"tags":[]},"outputs":[],"source":["# met = []\n","# a = np.array([1,2,3])\n","# b = np.array([2,3,4])\n","# met += list(a >= 2)\n","# print(met)\n","# met += list(b >= 2)\n","# print(met)\n","# target = [0, 1, 1, 0, 1, 1]\n","# s = f1_score(target, met)\n","# print(s)\n"]},{"metadata":{"trusted":true},"cell_type":"code","source":["train_df = pd.read_csv(\"../input/trends-assessment-prediction/train_scores.csv\")\n","drop_cols = [\"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]\n","train_df.drop(drop_cols, axis=1, inplace=True)\n","train_df[\"is_train\"] = True\n","# train_df[\"kfold\"] = df[\"kfold\"].astype(int)\n","\n","fnc = pd.read_csv(\"../input/trends-assessment-prediction/fnc.csv\")\n","fnc.fillna(fnc.mean(), inplace=True)\n","fnc = fnc.merge(train_df, on=\"Id\", how=\"left\")\n","test_fnc = fnc[fnc[\"is_train\"] != True].copy().reset_index(drop=True)\n","fnc = fnc[fnc[\"is_train\"] == True].copy().reset_index(drop=True)\n","\n","loading = pd.read_csv(\"../input/trends-assessment-prediction/loading.csv\")\n","loading.fillna(loading.mean(), inplace=True)\n","loading = loading.merge(train_df, on=\"Id\", how=\"left\")\n","test_loading = loading[loading[\"is_train\"] != True].copy().reset_index(drop=True)\n","loading = loading[loading[\"is_train\"] == True].copy().reset_index(drop=True)\n","kf = KFold(n_splits=config.num_folds, shuffle=True, random_state=config.seed)\n","for fold, (trn_, val_) in enumerate(kf.split(train_df)):\n","    loading.loc[val_, 'kfold'] = fold\n","    fnc.loc[val_, 'kfold'] = fold\n","loading = loading[loading[\"kfold\"] == config.fold].reset_index(drop=True)\n","fnc = fnc[fnc[\"kfold\"] == config.fold].reset_index(drop=True)"],"execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":20,"metadata":{"tags":[]},"outputs":[],"source":["# devide test data site2 and unknow\n","site2_id = pd.read_csv(\"../input/trends-assessment-prediction/reveal_ID_site2.csv\")\n","site2_id['is_site2'] = 1\n","loading[\"is_site2\"] = 0\n","fnc[\"is_site2\"] = 0\n","\n","# kf = StratifiedKFold(n_splits=config.num_folds, shuffle=True, random_state=config.seed)\n","# for fold, (trn_, val_) in enumerate(kf.split(adversal_loading_df[adversal_loading_df.columns[1:-1]], adversal_loading_df[\"is_site2\"].astype(int))):\n","#     adversal_loading_df.loc[val_, 'kfold'] = fold\n","#     adversal_fnc_df.loc[val_, 'kfold'] = fold\n","\n","# adversal_loading_df = adversal_loading_df[adversal_loading_df[\"kfold\"]==config.fold]\n","# adversal_fnc_df = adversal_fnc_df[adversal_fnc_df[\"kfold\"]==config.fold]\n","\n","test_loading = pd.merge(test_loading, site2_id, how=\"left\")\n","test_loading.loc[test_loading[\"is_site2\"] != 1, [\"is_site2\"]] = \"unknow\"\n","test_fnc = pd.merge(test_fnc, site2_id, how=\"left\")\n","test_fnc.loc[test_fnc[\"is_site2\"] != 1, [\"is_site2\"]] = \"unknow\"\n","\n","adversal_loading_df = pd.concat([loading, test_loading[test_loading[\"is_site2\"] == 1]], axis=0).reset_index(drop=True).drop(\"is_train\", axis=1)\n","adversal_fnc_df = pd.concat([fnc, test_fnc[test_fnc[\"is_site2\"] == 1]], axis=0).reset_index(drop=True).drop(\"is_train\", axis=1)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"         Id  SCN(53)_vs_SCN(69)  SCN(98)_vs_SCN(69)  SCN(99)_vs_SCN(69)  \\\n0     10003            0.343415            0.109974            0.741641   \n1     10006            0.323260            0.117238            0.640690   \n2     10010           -0.226350           -0.043393            0.334355   \n3     10011            0.167377            0.166101            0.521025   \n5     10013            0.583232            0.555189            0.834822   \n...     ...                 ...                 ...                 ...   \n5871  21743            0.390405            0.221638            0.621771   \n5872  21745            0.153193            0.287633            0.452419   \n5873  21748           -0.295271            0.108850            0.325825   \n5875  21751            0.455052            0.483856            0.589565   \n5876  21753            0.051042            0.088581            0.551354   \n\n      SCN(45)_vs_SCN(69)  ADN(21)_vs_SCN(69)  ADN(56)_vs_SCN(69)  \\\n0               0.578558           -0.676446           -0.436960   \n1               0.320641           -0.319674           -0.302980   \n2               0.319022           -0.170916           -0.259497   \n3               0.405185           -0.155573           -0.038033   \n5               0.730794           -0.395380           -0.314933   \n...                  ...                 ...                 ...   \n5871            0.638183           -0.435192           -0.175604   \n5872            0.615736           -0.271541           -0.028437   \n5873            0.737253           -0.322191           -0.063354   \n5875            0.633691            0.161995           -0.175318   \n5876            0.305542           -0.034378           -0.094778   \n\n      SMN(3)_vs_SCN(69)  SMN(9)_vs_SCN(69)  SMN(2)_vs_SCN(69)  ...  \\\n0             -0.295663          -0.377790          -0.344963  ...   \n1             -0.214006          -0.356314          -0.235281  ...   \n2             -0.139280           0.034884           0.090345  ...   \n3             -0.051752           0.051793           0.087029  ...   \n5             -0.692369          -0.650187          -0.554456  ...   \n...                 ...                ...                ...  ...   \n5871          -0.374706          -0.567474          -0.437886  ...   \n5872           0.037924           0.366023           0.305707  ...   \n5873          -0.174346          -0.097420          -0.074920  ...   \n5875           0.015480           0.135612           0.172296  ...   \n5876          -0.089954          -0.222876          -0.197503  ...   \n\n      CBN(4)_vs_DMN(94)  CBN(7)_vs_DMN(94)  CBN(18)_vs_CBN(13)  \\\n0             -0.022361           0.137625            0.677972   \n1              0.270419           0.367692            0.354501   \n2              0.193523           0.192254            0.563982   \n3              0.042626           0.179456            0.416546   \n5              0.173899          -0.112612            0.742858   \n...                 ...                ...                 ...   \n5871          -0.109941           0.289195            0.707932   \n5872           0.202241           0.254870            0.458581   \n5873           0.138985           0.343382            0.708744   \n5875          -0.047932           0.022317            0.583869   \n5876           0.096594           0.317651            0.545878   \n\n      CBN(4)_vs_CBN(13)  CBN(7)_vs_CBN(13)  CBN(4)_vs_CBN(18)  \\\n0              0.409412           0.563892           0.438684   \n1              0.486364           0.416908           0.285274   \n2              0.124482           0.488926           0.083368   \n3              0.445402           0.436909           0.165182   \n5              0.509233           0.620552           0.216062   \n...                 ...                ...                ...   \n5871           0.654735           0.549079           0.321413   \n5872           0.434638           0.587167           0.009854   \n5873           0.312812           0.536501           0.214803   \n5875           0.596734           0.515209           0.379589   \n5876           0.411197           0.230889           0.167354   \n\n      CBN(7)_vs_CBN(18)  CBN(7)_vs_CBN(4)  is_train  is_site2  \n0              0.618204          0.284474       NaN    unknow  \n1              0.693490          0.448526       NaN    unknow  \n2              0.774299          0.129327       NaN    unknow  \n3              0.591561          0.306678       NaN    unknow  \n5              0.781190          0.313136       NaN    unknow  \n...                 ...               ...       ...       ...  \n5871           0.599476          0.488016       NaN    unknow  \n5872           0.472956          0.342085       NaN    unknow  \n5873           0.849512          0.204741       NaN    unknow  \n5875           0.568422          0.439016       NaN    unknow  \n5876           0.221668          0.371357       NaN    unknow  \n\n[5367 rows x 1381 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>SCN(53)_vs_SCN(69)</th>\n      <th>SCN(98)_vs_SCN(69)</th>\n      <th>SCN(99)_vs_SCN(69)</th>\n      <th>SCN(45)_vs_SCN(69)</th>\n      <th>ADN(21)_vs_SCN(69)</th>\n      <th>ADN(56)_vs_SCN(69)</th>\n      <th>SMN(3)_vs_SCN(69)</th>\n      <th>SMN(9)_vs_SCN(69)</th>\n      <th>SMN(2)_vs_SCN(69)</th>\n      <th>...</th>\n      <th>CBN(4)_vs_DMN(94)</th>\n      <th>CBN(7)_vs_DMN(94)</th>\n      <th>CBN(18)_vs_CBN(13)</th>\n      <th>CBN(4)_vs_CBN(13)</th>\n      <th>CBN(7)_vs_CBN(13)</th>\n      <th>CBN(4)_vs_CBN(18)</th>\n      <th>CBN(7)_vs_CBN(18)</th>\n      <th>CBN(7)_vs_CBN(4)</th>\n      <th>is_train</th>\n      <th>is_site2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10003</td>\n      <td>0.343415</td>\n      <td>0.109974</td>\n      <td>0.741641</td>\n      <td>0.578558</td>\n      <td>-0.676446</td>\n      <td>-0.436960</td>\n      <td>-0.295663</td>\n      <td>-0.377790</td>\n      <td>-0.344963</td>\n      <td>...</td>\n      <td>-0.022361</td>\n      <td>0.137625</td>\n      <td>0.677972</td>\n      <td>0.409412</td>\n      <td>0.563892</td>\n      <td>0.438684</td>\n      <td>0.618204</td>\n      <td>0.284474</td>\n      <td>NaN</td>\n      <td>unknow</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10006</td>\n      <td>0.323260</td>\n      <td>0.117238</td>\n      <td>0.640690</td>\n      <td>0.320641</td>\n      <td>-0.319674</td>\n      <td>-0.302980</td>\n      <td>-0.214006</td>\n      <td>-0.356314</td>\n      <td>-0.235281</td>\n      <td>...</td>\n      <td>0.270419</td>\n      <td>0.367692</td>\n      <td>0.354501</td>\n      <td>0.486364</td>\n      <td>0.416908</td>\n      <td>0.285274</td>\n      <td>0.693490</td>\n      <td>0.448526</td>\n      <td>NaN</td>\n      <td>unknow</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10010</td>\n      <td>-0.226350</td>\n      <td>-0.043393</td>\n      <td>0.334355</td>\n      <td>0.319022</td>\n      <td>-0.170916</td>\n      <td>-0.259497</td>\n      <td>-0.139280</td>\n      <td>0.034884</td>\n      <td>0.090345</td>\n      <td>...</td>\n      <td>0.193523</td>\n      <td>0.192254</td>\n      <td>0.563982</td>\n      <td>0.124482</td>\n      <td>0.488926</td>\n      <td>0.083368</td>\n      <td>0.774299</td>\n      <td>0.129327</td>\n      <td>NaN</td>\n      <td>unknow</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10011</td>\n      <td>0.167377</td>\n      <td>0.166101</td>\n      <td>0.521025</td>\n      <td>0.405185</td>\n      <td>-0.155573</td>\n      <td>-0.038033</td>\n      <td>-0.051752</td>\n      <td>0.051793</td>\n      <td>0.087029</td>\n      <td>...</td>\n      <td>0.042626</td>\n      <td>0.179456</td>\n      <td>0.416546</td>\n      <td>0.445402</td>\n      <td>0.436909</td>\n      <td>0.165182</td>\n      <td>0.591561</td>\n      <td>0.306678</td>\n      <td>NaN</td>\n      <td>unknow</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>10013</td>\n      <td>0.583232</td>\n      <td>0.555189</td>\n      <td>0.834822</td>\n      <td>0.730794</td>\n      <td>-0.395380</td>\n      <td>-0.314933</td>\n      <td>-0.692369</td>\n      <td>-0.650187</td>\n      <td>-0.554456</td>\n      <td>...</td>\n      <td>0.173899</td>\n      <td>-0.112612</td>\n      <td>0.742858</td>\n      <td>0.509233</td>\n      <td>0.620552</td>\n      <td>0.216062</td>\n      <td>0.781190</td>\n      <td>0.313136</td>\n      <td>NaN</td>\n      <td>unknow</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5871</th>\n      <td>21743</td>\n      <td>0.390405</td>\n      <td>0.221638</td>\n      <td>0.621771</td>\n      <td>0.638183</td>\n      <td>-0.435192</td>\n      <td>-0.175604</td>\n      <td>-0.374706</td>\n      <td>-0.567474</td>\n      <td>-0.437886</td>\n      <td>...</td>\n      <td>-0.109941</td>\n      <td>0.289195</td>\n      <td>0.707932</td>\n      <td>0.654735</td>\n      <td>0.549079</td>\n      <td>0.321413</td>\n      <td>0.599476</td>\n      <td>0.488016</td>\n      <td>NaN</td>\n      <td>unknow</td>\n    </tr>\n    <tr>\n      <th>5872</th>\n      <td>21745</td>\n      <td>0.153193</td>\n      <td>0.287633</td>\n      <td>0.452419</td>\n      <td>0.615736</td>\n      <td>-0.271541</td>\n      <td>-0.028437</td>\n      <td>0.037924</td>\n      <td>0.366023</td>\n      <td>0.305707</td>\n      <td>...</td>\n      <td>0.202241</td>\n      <td>0.254870</td>\n      <td>0.458581</td>\n      <td>0.434638</td>\n      <td>0.587167</td>\n      <td>0.009854</td>\n      <td>0.472956</td>\n      <td>0.342085</td>\n      <td>NaN</td>\n      <td>unknow</td>\n    </tr>\n    <tr>\n      <th>5873</th>\n      <td>21748</td>\n      <td>-0.295271</td>\n      <td>0.108850</td>\n      <td>0.325825</td>\n      <td>0.737253</td>\n      <td>-0.322191</td>\n      <td>-0.063354</td>\n      <td>-0.174346</td>\n      <td>-0.097420</td>\n      <td>-0.074920</td>\n      <td>...</td>\n      <td>0.138985</td>\n      <td>0.343382</td>\n      <td>0.708744</td>\n      <td>0.312812</td>\n      <td>0.536501</td>\n      <td>0.214803</td>\n      <td>0.849512</td>\n      <td>0.204741</td>\n      <td>NaN</td>\n      <td>unknow</td>\n    </tr>\n    <tr>\n      <th>5875</th>\n      <td>21751</td>\n      <td>0.455052</td>\n      <td>0.483856</td>\n      <td>0.589565</td>\n      <td>0.633691</td>\n      <td>0.161995</td>\n      <td>-0.175318</td>\n      <td>0.015480</td>\n      <td>0.135612</td>\n      <td>0.172296</td>\n      <td>...</td>\n      <td>-0.047932</td>\n      <td>0.022317</td>\n      <td>0.583869</td>\n      <td>0.596734</td>\n      <td>0.515209</td>\n      <td>0.379589</td>\n      <td>0.568422</td>\n      <td>0.439016</td>\n      <td>NaN</td>\n      <td>unknow</td>\n    </tr>\n    <tr>\n      <th>5876</th>\n      <td>21753</td>\n      <td>0.051042</td>\n      <td>0.088581</td>\n      <td>0.551354</td>\n      <td>0.305542</td>\n      <td>-0.034378</td>\n      <td>-0.094778</td>\n      <td>-0.089954</td>\n      <td>-0.222876</td>\n      <td>-0.197503</td>\n      <td>...</td>\n      <td>0.096594</td>\n      <td>0.317651</td>\n      <td>0.545878</td>\n      <td>0.411197</td>\n      <td>0.230889</td>\n      <td>0.167354</td>\n      <td>0.221668</td>\n      <td>0.371357</td>\n      <td>NaN</td>\n      <td>unknow</td>\n    </tr>\n  </tbody>\n</table>\n<p>5367 rows × 1381 columns</p>\n</div>"},"metadata":{},"execution_count":21}],"source":["test_loading_uk = test_loading[test_loading[\"is_site2\"]==\"unknow\"]\n","test_fnc_uk = test_fnc[test_fnc[\"is_site2\"]==\"unknow\"]\n","test_fnc_uk"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["adversal_loading_df['kfold'] = -1\n","adversal_fnc_df['kfold'] = -1\n","\n","kf = StratifiedKFold(n_splits=config.num_folds, shuffle=True, random_state=config.seed)\n","for fold, (trn_, val_) in enumerate(kf.split(adversal_loading_df[adversal_loading_df.columns[1:-1]], adversal_loading_df[\"is_site2\"].astype(int))):\n","    adversal_loading_df.loc[val_, 'kfold'] = fold\n","    adversal_fnc_df.loc[val_, 'kfold'] = fold\n","\n","# adversal_loading_df = adversal_loading_df[adversal_loading_df[\"kfold\"]!=config.fold]\n","# adversal_fnc_df = adversal_fnc_df[adversal_fnc_df[\"kfold\"]!=config.fold]"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"Id          1686\nis_site2       2\nkfold          5\npath        1686\ndtype: int64"},"metadata":{},"execution_count":23}],"source":["adversal_target_df = adversal_loading_df[[\"Id\", \"is_site2\", \"kfold\"]]\n","adversal_target_df.loc[:, 'path'] = -1\n","id_names = adversal_target_df.loc[adversal_target_df[\"is_site2\"]==0, [\"Id\"]].values.astype(str)\n","adversal_target_df.loc[adversal_target_df['is_site2']==0, ['path']] = [f\"{config.root_train_path}/{id_name}.mat\" for id_name in list(id_names.squeeze())]\n","id_names = adversal_target_df.loc[adversal_target_df[\"is_site2\"]==1, [\"Id\"]].values.astype(str)\n","adversal_target_df.loc[adversal_target_df['is_site2']==1, ['path']] = [f\"{config.root_test_path}/{id_name}.mat\" for id_name in list(id_names.squeeze())]\n","adversal_target_df.nunique()"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"         Id     IC_01     IC_07     IC_05     IC_16     IC_26     IC_06  \\\n0     10002  0.009087  0.009291  0.007049 -0.002076 -0.002227  0.004605   \n1     10008  0.007745  0.009748  0.009356 -0.004219 -0.003852  0.012024   \n2     10025  0.005216  0.002440  0.008248  0.002507 -0.001299  0.013204   \n3     10040  0.013076  0.009528  0.015875 -0.001391 -0.002204  0.016201   \n4     10044  0.003747  0.012354  0.019075  0.005545  0.000610  0.014962   \n...     ...       ...       ...       ...       ...       ...       ...   \n1681  21658  0.009104  0.007580  0.009138 -0.000451 -0.006240  0.013207   \n1682  21675  0.014752  0.008878  0.006876 -0.000607  0.000041  0.014623   \n1683  21678  0.005410 -0.002808  0.004156  0.002436 -0.003584  0.008285   \n1684  21686  0.007964  0.012347  0.014135 -0.003312 -0.008608  0.015035   \n1685  21749  0.004783  0.017910  0.012128 -0.005683 -0.011613  0.017000   \n\n         IC_10     IC_09     IC_18  ...     IC_21     IC_28     IC_11  \\\n0     0.012277  0.002946  0.004086  ...  0.012004 -0.011814  0.022479   \n1     0.010205  0.002903  0.000870  ...  0.014109 -0.006456  0.025408   \n2     0.022979  0.011265  0.009722  ...  0.000198 -0.012289  0.032331   \n3     0.011301 -0.001545  0.007734  ...  0.020134 -0.011959  0.020192   \n4     0.018568 -0.002129 -0.005763  ...  0.010731 -0.008261  0.021998   \n...        ...       ...       ...  ...       ...       ...       ...   \n1681  0.010697  0.004401 -0.008484  ...  0.017380 -0.003840  0.028423   \n1682  0.011341  0.003492  0.007985  ...  0.013210 -0.009058  0.020173   \n1683  0.019236  0.006632  0.001594  ...  0.012144 -0.008436  0.025899   \n1684  0.015908 -0.002958  0.005207  ...  0.014008 -0.002648  0.027892   \n1685  0.007230  0.001315  0.008788  ...  0.016017 -0.003296  0.018724   \n\n         IC_20     IC_30     IC_22     IC_29     IC_14  kfold  is_site2  \n0     0.005739  0.002880 -0.016609  0.025543  0.014524      0         0  \n1     0.004483  0.000688 -0.013822  0.029328  0.010936      0         0  \n2     0.005606  0.004954 -0.024609  0.031660  0.021196      3         0  \n3     0.002023 -0.001686 -0.010606  0.029586  0.015970      4         0  \n4     0.008636  0.002120 -0.028707  0.038849  0.022879      4         0  \n...        ...       ...       ...       ...       ...    ...       ...  \n1681  0.006983 -0.000067 -0.019470  0.027933  0.021043      4         1  \n1682  0.005618  0.002111 -0.013285  0.025771  0.013120      0         1  \n1683  0.003901  0.003781 -0.029672  0.028765  0.018470      4         1  \n1684  0.016234  0.004111 -0.008403  0.030550  0.015147      0         1  \n1685  0.008006 -0.000521 -0.001333  0.029249  0.015356      3         1  \n\n[1686 rows x 29 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>IC_01</th>\n      <th>IC_07</th>\n      <th>IC_05</th>\n      <th>IC_16</th>\n      <th>IC_26</th>\n      <th>IC_06</th>\n      <th>IC_10</th>\n      <th>IC_09</th>\n      <th>IC_18</th>\n      <th>...</th>\n      <th>IC_21</th>\n      <th>IC_28</th>\n      <th>IC_11</th>\n      <th>IC_20</th>\n      <th>IC_30</th>\n      <th>IC_22</th>\n      <th>IC_29</th>\n      <th>IC_14</th>\n      <th>kfold</th>\n      <th>is_site2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10002</td>\n      <td>0.009087</td>\n      <td>0.009291</td>\n      <td>0.007049</td>\n      <td>-0.002076</td>\n      <td>-0.002227</td>\n      <td>0.004605</td>\n      <td>0.012277</td>\n      <td>0.002946</td>\n      <td>0.004086</td>\n      <td>...</td>\n      <td>0.012004</td>\n      <td>-0.011814</td>\n      <td>0.022479</td>\n      <td>0.005739</td>\n      <td>0.002880</td>\n      <td>-0.016609</td>\n      <td>0.025543</td>\n      <td>0.014524</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10008</td>\n      <td>0.007745</td>\n      <td>0.009748</td>\n      <td>0.009356</td>\n      <td>-0.004219</td>\n      <td>-0.003852</td>\n      <td>0.012024</td>\n      <td>0.010205</td>\n      <td>0.002903</td>\n      <td>0.000870</td>\n      <td>...</td>\n      <td>0.014109</td>\n      <td>-0.006456</td>\n      <td>0.025408</td>\n      <td>0.004483</td>\n      <td>0.000688</td>\n      <td>-0.013822</td>\n      <td>0.029328</td>\n      <td>0.010936</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10025</td>\n      <td>0.005216</td>\n      <td>0.002440</td>\n      <td>0.008248</td>\n      <td>0.002507</td>\n      <td>-0.001299</td>\n      <td>0.013204</td>\n      <td>0.022979</td>\n      <td>0.011265</td>\n      <td>0.009722</td>\n      <td>...</td>\n      <td>0.000198</td>\n      <td>-0.012289</td>\n      <td>0.032331</td>\n      <td>0.005606</td>\n      <td>0.004954</td>\n      <td>-0.024609</td>\n      <td>0.031660</td>\n      <td>0.021196</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10040</td>\n      <td>0.013076</td>\n      <td>0.009528</td>\n      <td>0.015875</td>\n      <td>-0.001391</td>\n      <td>-0.002204</td>\n      <td>0.016201</td>\n      <td>0.011301</td>\n      <td>-0.001545</td>\n      <td>0.007734</td>\n      <td>...</td>\n      <td>0.020134</td>\n      <td>-0.011959</td>\n      <td>0.020192</td>\n      <td>0.002023</td>\n      <td>-0.001686</td>\n      <td>-0.010606</td>\n      <td>0.029586</td>\n      <td>0.015970</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10044</td>\n      <td>0.003747</td>\n      <td>0.012354</td>\n      <td>0.019075</td>\n      <td>0.005545</td>\n      <td>0.000610</td>\n      <td>0.014962</td>\n      <td>0.018568</td>\n      <td>-0.002129</td>\n      <td>-0.005763</td>\n      <td>...</td>\n      <td>0.010731</td>\n      <td>-0.008261</td>\n      <td>0.021998</td>\n      <td>0.008636</td>\n      <td>0.002120</td>\n      <td>-0.028707</td>\n      <td>0.038849</td>\n      <td>0.022879</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1681</th>\n      <td>21658</td>\n      <td>0.009104</td>\n      <td>0.007580</td>\n      <td>0.009138</td>\n      <td>-0.000451</td>\n      <td>-0.006240</td>\n      <td>0.013207</td>\n      <td>0.010697</td>\n      <td>0.004401</td>\n      <td>-0.008484</td>\n      <td>...</td>\n      <td>0.017380</td>\n      <td>-0.003840</td>\n      <td>0.028423</td>\n      <td>0.006983</td>\n      <td>-0.000067</td>\n      <td>-0.019470</td>\n      <td>0.027933</td>\n      <td>0.021043</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1682</th>\n      <td>21675</td>\n      <td>0.014752</td>\n      <td>0.008878</td>\n      <td>0.006876</td>\n      <td>-0.000607</td>\n      <td>0.000041</td>\n      <td>0.014623</td>\n      <td>0.011341</td>\n      <td>0.003492</td>\n      <td>0.007985</td>\n      <td>...</td>\n      <td>0.013210</td>\n      <td>-0.009058</td>\n      <td>0.020173</td>\n      <td>0.005618</td>\n      <td>0.002111</td>\n      <td>-0.013285</td>\n      <td>0.025771</td>\n      <td>0.013120</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1683</th>\n      <td>21678</td>\n      <td>0.005410</td>\n      <td>-0.002808</td>\n      <td>0.004156</td>\n      <td>0.002436</td>\n      <td>-0.003584</td>\n      <td>0.008285</td>\n      <td>0.019236</td>\n      <td>0.006632</td>\n      <td>0.001594</td>\n      <td>...</td>\n      <td>0.012144</td>\n      <td>-0.008436</td>\n      <td>0.025899</td>\n      <td>0.003901</td>\n      <td>0.003781</td>\n      <td>-0.029672</td>\n      <td>0.028765</td>\n      <td>0.018470</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1684</th>\n      <td>21686</td>\n      <td>0.007964</td>\n      <td>0.012347</td>\n      <td>0.014135</td>\n      <td>-0.003312</td>\n      <td>-0.008608</td>\n      <td>0.015035</td>\n      <td>0.015908</td>\n      <td>-0.002958</td>\n      <td>0.005207</td>\n      <td>...</td>\n      <td>0.014008</td>\n      <td>-0.002648</td>\n      <td>0.027892</td>\n      <td>0.016234</td>\n      <td>0.004111</td>\n      <td>-0.008403</td>\n      <td>0.030550</td>\n      <td>0.015147</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1685</th>\n      <td>21749</td>\n      <td>0.004783</td>\n      <td>0.017910</td>\n      <td>0.012128</td>\n      <td>-0.005683</td>\n      <td>-0.011613</td>\n      <td>0.017000</td>\n      <td>0.007230</td>\n      <td>0.001315</td>\n      <td>0.008788</td>\n      <td>...</td>\n      <td>0.016017</td>\n      <td>-0.003296</td>\n      <td>0.018724</td>\n      <td>0.008006</td>\n      <td>-0.000521</td>\n      <td>-0.001333</td>\n      <td>0.029249</td>\n      <td>0.015356</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>1686 rows × 29 columns</p>\n</div>"},"metadata":{},"execution_count":25}],"source":["# fMRI_train_path = \n","# fMRI_test_path = \n","adversal_loading_df#.iloc[:,1:-2]"]},{"metadata":{},"cell_type":"markdown","source":["# Running on multiple folds"]},{"metadata":{"trusted":true,"tags":[]},"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","save_model_path = \"models/adversal_resnet10.pth\"\n","log_path = \"logs/log_adversal_resnet10.csv\"\n","print(device)\n","print(save_model_path)\n","print(log_path)"],"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":"cuda\nadversal_resnet10.pth\nlog_adversal_resnet10.csv\n"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["def run(fold):\n","    \n","    model = TReNDSModel()\n","    model.to(device)\n","    \n","    adversal_train_loading_df = adversal_loading_df[adversal_loading_df['kfold'] != fold].reset_index(drop=True)\n","    adversal_valid_loading_df = adversal_loading_df[adversal_loading_df['kfold'] == fold].reset_index(drop=True)\n","    adversal_train_fnc_df = adversal_fnc_df[adversal_fnc_df['kfold'] != fold].reset_index(drop=True)\n","    adversal_valid_fnc_df = adversal_fnc_df[adversal_fnc_df['kfold'] == fold].reset_index(drop=True)\n","    adversal_target_train_df = adversal_target_df[adversal_target_df['kfold'] != fold].reset_index(drop=True)\n","    adversal_target_valid_df = adversal_target_df[adversal_target_df['kfold'] != fold].reset_index(drop=True)\n","    \n","    train_dataset = MRIMapDataset(df=adversal_target_train_df, fnc=adversal_train_fnc_df, loading=adversal_train_loading_df, \n","    fMRI_path=adversal_target_train_df[\"path\"],mode=\"train\")\n","    valid_dataset = MRIMapDataset(df=adversal_target_valid_df, fnc=adversal_valid_fnc_df, loading=adversal_valid_loading_df, \n","    fMRI_path=adversal_target_valid_df[\"path\"], mode=\"train\")\n","    \n","    train_data_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=config.batch_size,\n","        num_workers=config.num_workers,\n","        shuffle=True\n","    )\n","    \n","    valid_data_loader = torch.utils.data.DataLoader(\n","        valid_dataset,\n","        batch_size=config.batch_size,\n","        num_workers=config.num_workers,\n","        shuffle=False\n","    )\n","    \n","    fitter = GPUFitter(model, fold, device, config, save_model_path=save_model_path, log_path=log_path)\n","    fitter.fit(train_data_loader, valid_data_loader)\n","    print('over')\n","    return fitter"],"execution_count":82,"outputs":[]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":["adversal_train_loading_df = adversal_loading_df[adversal_loading_df['kfold'] != fold].reset_index(drop=True)\n","adversal_valid_loading_df = adversal_loading_df[adversal_loading_df['kfold'] == fold].reset_index(drop=True)\n","adversal_train_fnc_df = adversal_fnc_df[adversal_fnc_df['kfold'] != fold].reset_index(drop=True)\n","adversal_valid_fnc_df = adversal_fnc_df[adversal_fnc_df['kfold'] == fold].reset_index(drop=True)\n","adversal_target_train_df = adversal_target_df[adversal_target_df['kfold'] != fold].reset_index(drop=True)\n","adversal_target_valid_df = adversal_target_df[adversal_target_df['kfold'] == fold].reset_index(drop=True)\n","\n","train_dataset = MRIMapDataset(df=adversal_target_train_df, fnc=adversal_train_fnc_df, loading=adversal_train_loading_df, fMRI_path=adversal_target_train_df['path'], mode=\"train\")\n","valid_dataset = MRIMapDataset(df=adversal_target_valid_df, fnc=adversal_valid_fnc_df, loading=adversal_valid_loading_df, fMRI_path=adversal_target_valid_df['path'], mode=\"train\")\n","train_data_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=config.batch_size,\n","        num_workers=config.num_workers,\n","        shuffle=True)\n"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"         Id is_site2  kfold                                               path\n0     10001        0      0  ../input/trends-assessment-prediction/fMRI_tra...\n1     10005        0      0  ../input/trends-assessment-prediction/fMRI_tra...\n2     10041        0      3  ../input/trends-assessment-prediction/fMRI_tra...\n3     10073        0      3  ../input/trends-assessment-prediction/fMRI_tra...\n4     10090        0      3  ../input/trends-assessment-prediction/fMRI_tra...\n...     ...      ...    ...                                                ...\n1343  21549        1      0  ../input/trends-assessment-prediction/fMRI_tes...\n1344  21626        1      1  ../input/trends-assessment-prediction/fMRI_tes...\n1345  21675        1      0  ../input/trends-assessment-prediction/fMRI_tes...\n1346  21686        1      0  ../input/trends-assessment-prediction/fMRI_tes...\n1347  21749        1      3  ../input/trends-assessment-prediction/fMRI_tes...\n\n[1348 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>is_site2</th>\n      <th>kfold</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10001</td>\n      <td>0</td>\n      <td>0</td>\n      <td>../input/trends-assessment-prediction/fMRI_tra...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10005</td>\n      <td>0</td>\n      <td>0</td>\n      <td>../input/trends-assessment-prediction/fMRI_tra...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10041</td>\n      <td>0</td>\n      <td>3</td>\n      <td>../input/trends-assessment-prediction/fMRI_tra...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10073</td>\n      <td>0</td>\n      <td>3</td>\n      <td>../input/trends-assessment-prediction/fMRI_tra...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10090</td>\n      <td>0</td>\n      <td>3</td>\n      <td>../input/trends-assessment-prediction/fMRI_tra...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1343</th>\n      <td>21549</td>\n      <td>1</td>\n      <td>0</td>\n      <td>../input/trends-assessment-prediction/fMRI_tes...</td>\n    </tr>\n    <tr>\n      <th>1344</th>\n      <td>21626</td>\n      <td>1</td>\n      <td>1</td>\n      <td>../input/trends-assessment-prediction/fMRI_tes...</td>\n    </tr>\n    <tr>\n      <th>1345</th>\n      <td>21675</td>\n      <td>1</td>\n      <td>0</td>\n      <td>../input/trends-assessment-prediction/fMRI_tes...</td>\n    </tr>\n    <tr>\n      <th>1346</th>\n      <td>21686</td>\n      <td>1</td>\n      <td>0</td>\n      <td>../input/trends-assessment-prediction/fMRI_tes...</td>\n    </tr>\n    <tr>\n      <th>1347</th>\n      <td>21749</td>\n      <td>1</td>\n      <td>3</td>\n      <td>../input/trends-assessment-prediction/fMRI_tes...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1348 rows × 4 columns</p>\n</div>"},"metadata":{},"execution_count":81}],"source":["# valid_dataset.__len__()\n","adversal_target_train_df #5110 rows × 4 columns\n","# adversal_target_valid_df"]},{"metadata":{"trusted":true},"cell_type":"code","source":["fitter = run(config.fold)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["log_df = pd.read_csv(fitter.log_path)\n","# log_df = pd.read_csv(\"log0.csv\")\n","# log_df = pd.read_csv('../input/trend3dcnn/log0.csv')\n","\n","log_df"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["ptcture_path = log_path[:-4] +f\"_fold{config.fold}_No{file_No}.png\"\n","plt.figure(figsize=(15,5))\n","plt.title(\"loss\")\n","plt.subplot(1,2,1)\n","log_df.loss.plot()\n","log_df.val_loss.plot()\n","plt.subplot(1,2,2)\n","plt.title(\"score\")\n","log_df.score.plot()\n","log_df.val_score.plot()\n","plt.savefig(f\"pictures/{ptcture_path}\")"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# train_df = df[df['kfold'] != fold].reset_index(drop=True)\n","# train_dataset = MRIMapDataset(df=train_df, mode=\"train\")\n","# train_dataset[0][\"scan_maps\"].shape"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Parallel(n_jobs=config.num_folds, backend=\"threading\")(delayed(run)(i) for i in range(config.num_folds))"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# valid data and test data "]},{"metadata":{"trusted":true},"cell_type":"code","source":["# resnet10でだいたい45分 num_worker=0のとき\n","# adversal_valid_fnc_df = adversal_fnc_df[adversal_fnc_df['kfold'] == config.fold].reset_index(drop=True)\n","# adversal_valid_loading_df = adversal_target_df[adversal_loading_df['kfold'] == config.fold].reset_index(drop=True)\n","\n","# valid_dataset = MRIMapDataset(fnc=adversal_valid_fnc_df, loading=adversal_valid_loading_df, mode=\"test\")\n","# valid_data_loader = torch.utils.data.DataLoader(\n","#     valid_dataset,\n","#     batch_size=config.batch_size,\n","#     num_workers=config.num_workers,\n","#     shuffle=False)\n","# _test_loading = test_loading.loc[test_loading[\"is_site2\"] != 1]\n","# _test_fnc = test_fnc.loc[test_fnc[\"is_site2\"] != 1]\n","# test_loading_uk\n","test_dataset = MRIMapDataset(fnc=test_fnc_uk, loading=test_loading_uk, mode=\"test\", path=test_loading_uk[\"Id\"])\n","test_dataloader = torch.utils.data.DataLoader(\n","        test_dataset,\n","        batch_size=config.batch_size,\n","        num_workers=config.num_workers,\n","        shuffle=False)\n","model = TReNDSModel()\n","model.load_state_dict(torch.load(fitter.save_model_path))#'../input/trend3dcnn/checkpoint0.pth'\n","model.to(device)\n","model.eval()\n","\n","test_preds = np.empty((0, 2))\n","valid_preds = np.empty((0, 2))\n","with torch.no_grad():\n","    for step, data in enumerate(tqdm(test_dataloader)):\n","        scan_maps = data['scan_maps']\n","        fnc = data['fnc']\n","        loading =data['loading']       \n","        scan_maps = scan_maps.to(device, dtype=torch.float)\n","        fnc = fnc.to(device, dtype=torch.float)\n","        loading = loading.to(device, dtype=torch.float)\n","        \n","        outputs = model(scan_maps, fnc, loading)\n","        batch_size = scan_maps.size(0)\n","        outputs = outputs.detach().cpu().numpy()\n","        test_preds = np.concatenate([test_preds, outputs], 0)\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","    test_df = pd.DataFrame(test_preds, columns=[\"site1\", \"site2\"]])\n","    test_df.to_csv(f\"adversal/test_fold{config.fold}_No{file_No}.csv\", index=False)\n","    # for step, data in enumerate(tqdm(valid_data_loader)):\n","    #     scan_maps = data['scan_maps']\n","    #     fnc = data['fnc']\n","    #     loading =data['loading']       \n","    #     scan_maps = scan_maps.to(device, dtype=torch.float)\n","    #     fnc = fnc.to(device, dtype=torch.float)\n","    #     loading = loading.to(device, dtype=torch.float)\n","        \n","    #     outputs = model(scan_maps, fnc, loading)\n","    #     batch_size = scan_maps.size(0)\n","    #     outputs = outputs.detach().cpu().numpy()\n","    #     valid_preds = np.concatenate([valid_preds, outputs], 0)\n","    #     torch.cuda.empty_cache()\n","    #     gc.collect()\n","    # valid_df = pd.DataFrame(valid_preds, columns=[\"site1\", \"site2\"]])\n","    # valid_df.to_csv(f\"adversal/valid_fold{config.fold}_No{file_No}.csv\", index=False)\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["print(outputs.shape)\n","print(test_preds)\n","# print(valid_preds.shape)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# test_df = pd.DataFrame(test_preds, columns=[\"age\", \"domain1_var1\", \"domain1_var2\",\"domain2_var1\", \"domain2_var2\"])\n","# test_df = pd.DataFrame(test_preds, columns=[\"site1\", \"site2\"]])\n","# test_df.describe()\n","# test_df.to_csv(f\"test_fold{config.fold}_No{file_No}.csv\", index=False)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# list1 = os.listdir(config.root_test_path)\n","# list2 = sorted(list1)\n","# test_df[\"Id\"] = list2\n","# test_df[\"Id\"] = test_df[\"Id\"].map(lambda x: x[:-4])\n","# test_df.set_index(\"Id\", drop=True, inplace=True)\n","# test_df"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# df_long = test_df.stack().reset_index()\n","# df_long.rename(columns={'level_1': 'target', 0: 'Predicted'}, inplace=True)\n","# df_long[\"Id\"] = df_long[\"Id\"] + \"_\" + df_long[\"target\"]\n","# df_long.drop(\"target\", axis=1, inplace=True)\n","# df_long.to_csv('submission_No{file_No}.csv', index=False)"],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.8-final","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}