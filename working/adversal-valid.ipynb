{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":["file_No = 30\n","fold = 0"],"execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Importing dependencies"]},{"metadata":{"trusted":true},"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset\n","from sklearn.metrics import f1_score\n","import numpy as np\n","import pandas as pd\n","\n","import random\n","from tqdm.notebook import tqdm\n","import math\n","from functools import partial\n","import h5py\n","from datetime import datetime\n","import os\n","import time\n","import gc\n","from joblib import Parallel, delayed\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import KFold, StratifiedKFold\n","import nilearn as nl\n","import nilearn.plotting as nlplt\n","import nibabel as nib"],"execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Config"]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":["class config:\n","    epochs = 100\n","    batch_size = 16\n","    test_batch_size = 16\n","    learning_rate = 1e-3\n","    fMRI_mask_path = '../input/trends-assessment-prediction/fMRI_mask.nii'\n","    root_train_path = '../input/trends-assessment-prediction/fMRI_train'\n","    root_test_path = '../input/trends-assessment-prediction/fMRI_test'\n","    num_folds = 5\n","    seed = 2020\n","    verbose = True\n","    verbose_step = 1\n","    num_workers = 4\n","    test_num_workers = 4\n","    target = [\"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]\n","    weight = [0.3, 0.175, 0.175, 0.175, 0.175]\n","    # cross validationをするときはここでfoldを変更する\n","    fold = fold"],"execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"tags":[]},"cell_type":"code","source":["print(\"fold\",config.fold, \"file_No\", file_No)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":"fold 0 file_No 30\n"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","seed_everything(config.seed)"],"execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Metrics"]},{"metadata":{"trusted":true},"cell_type":"code","source":["class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","    \n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","    \n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Loss function"]},{"metadata":{"trusted":true,"tags":[]},"cell_type":"code","source":["def focal_loss(labels, logits, alpha, gamma):\n","    \"\"\"Compute the focal loss between `logits` and the ground truth `labels`.\n","    Focal loss = -alpha_t * (1-pt)^gamma * log(pt)\n","    where pt is the probability of being classified to the true class.\n","    pt = p (if true class), otherwise pt = 1 - p. p = sigmoid(logit).\n","    Args:\n","      labels: A float tensor of size [batch, num_classes].\n","      logits: A float tensor of size [batch, num_classes].\n","      alpha: A float tensor of size [batch_size]\n","        specifying per-example weight for balanced cross entropy.\n","      gamma: A float scalar modulating loss from hard and easy examples.\n","    Returns:\n","      focal_loss: A float32 scalar representing normalized total loss.\n","    \"\"\"    \n","    BCLoss = F.binary_cross_entropy_with_logits(input = logits, target = labels,reduction = \"none\")\n","\n","    if gamma == 0.0:\n","        modulator = 1.0\n","    else:\n","        modulator = torch.exp(-gamma * labels * logits - gamma * torch.log(1 + \n","            torch.exp(-1.0 * logits)))\n","\n","    loss = modulator * BCLoss\n","\n","    weighted_loss = alpha * loss\n","    focal_loss = torch.sum(weighted_loss)\n","\n","    focal_loss /= torch.sum(labels)\n","    return focal_loss\n","\n","\n","\n","def CB_loss(labels, logits, samples_per_cls, no_of_classes, loss_type, beta, gamma):\n","    \"\"\"Compute the Class Balanced Loss between `logits` and the ground truth `labels`.\n","    Class Balanced Loss: ((1-beta)/(1-beta^n))*Loss(labels, logits)\n","    where Loss is one of the standard losses used for Neural Networks.\n","    Args:\n","      labels: A int tensor of size [batch].\n","      logits: A float tensor of size [batch, no_of_classes].\n","      samples_per_cls: A python list of size [no_of_classes].\n","      no_of_classes: total number of classes. int\n","      loss_type: string. One of \"sigmoid\", \"focal\", \"softmax\".\n","      beta: float. Hyperparameter for Class balanced loss.\n","      gamma: float. Hyperparameter for Focal loss.\n","    Returns:\n","      cb_loss: A float tensor representing class balanced loss\n","    \"\"\"\n","    effective_num = 1.0 - np.power(beta, samples_per_cls)\n","    weights = (1.0 - beta) / np.array(effective_num)\n","    weights = weights / np.sum(weights) * no_of_classes\n","\n","    labels_one_hot = F.one_hot(labels, no_of_classes).float()\n","\n","    weights = torch.tensor(weights).float()\n","    weights = weights.unsqueeze(0)\n","    weights = weights.repeat(labels_one_hot.shape[0],1) * labels_one_hot\n","    weights = weights.sum(1)\n","    weights = weights.unsqueeze(1)\n","    weights = weights.repeat(1,no_of_classes)\n","\n","    if loss_type == \"focal\":\n","        cb_loss = focal_loss(labels_one_hot, logits, weights, gamma)\n","    elif loss_type == \"sigmoid\":\n","        cb_loss = F.binary_cross_entropy_with_logits(input = logits,target = labels_one_hot, weight = weights)\n","    elif loss_type == \"softmax\":\n","        pred = logits.softmax(dim = 1)\n","        cb_loss = F.binary_cross_entropy(input = pred, target = labels_one_hot, weight = weights)\n","    return cb_loss\n","no_of_classes = 2\n","logits = F.softmax(torch.rand(10,no_of_classes)).float()#.view(-1)\n","labels = torch.empty(10).random_(2).to(torch.int64)\n","print(logits)\n","print(labels)\n","beta = 0.9999\n","gamma = 1.0\n","samples_per_cls = [400, 4000]\n","loss_type = \"focal\"\n","cb_loss = CB_loss(labels, logits, samples_per_cls, no_of_classes,loss_type, beta, gamma)\n","print(cb_loss)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":"tensor([[0.5943, 0.4057],\n        [0.6159, 0.3841],\n        [0.5529, 0.4471],\n        [0.4451, 0.5549],\n        [0.4062, 0.5938],\n        [0.4346, 0.5654],\n        [0.5699, 0.4301],\n        [0.5822, 0.4178],\n        [0.6635, 0.3365],\n        [0.3180, 0.6820]])\ntensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0])\ntensor(1.1445)\n"}]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"tensor(1.1445)"},"metadata":{},"execution_count":12}],"source":["c = CB_loss\n","c(labels, logits, samples_per_cls, no_of_classes,loss_type, beta, gamma)"]},{"metadata":{},"cell_type":"markdown","source":["# Model"]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":["__all__ = [\n","    'resnet10', \n","    'resnet18', \n","    'resnet34', \n","    'resnet50', \n","    'resnet101',\n","    'resnet152', \n","    'resnet200'\n","]\n","\n","def conv3x3x3(in_planes, out_planes, stride=1, dilation=1):\n","    # 3x3x3 convolution with padding\n","    return nn.Conv3d(\n","        in_planes,\n","        out_planes,\n","        kernel_size=3,\n","        dilation=dilation,\n","        stride=stride,\n","        padding=dilation,\n","        bias=False)\n","\n","def downsample_basic_block(x, planes, stride, no_cuda=False):\n","    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n","    zero_pads = torch.Tensor(out.size(0), planes - out.size(1), out.size(2), out.size(3), out.size(4)).zero_()\n","    if not no_cuda:\n","        if isinstance(out.data, torch.cuda.FloatTensor):\n","            zero_pads = zero_pads.cuda()\n","\n","    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n","    return out\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3x3(inplanes, planes, stride=stride, dilation=dilation)\n","        self.bn1 = nn.BatchNorm3d(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3x3(planes, planes, dilation=dilation)\n","        self.bn2 = nn.BatchNorm3d(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.dilation = dilation\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm3d(planes)\n","        self.conv2 = nn.Conv3d(\n","            planes, planes, kernel_size=3, stride=stride, dilation=dilation, padding=dilation, bias=False)\n","        self.bn2 = nn.BatchNorm3d(planes)\n","        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm3d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.dilation = dilation\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","        return out\n","\n","class ResNet3D(nn.Module):\n","\n","    def __init__(self,\n","                 block,\n","                 layers,\n","                 shortcut_type='B',\n","                 num_class = 5,\n","                 no_cuda=False):\n","\n","        self.inplanes = 64\n","        self.no_cuda = no_cuda\n","        super(ResNet3D, self).__init__()\n","\n","        # 3D conv net\n","        self.conv1 = nn.Conv3d(53, 64, kernel_size=7, stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n","        # self.conv1 = nn.Conv3d(1, 64, kernel_size=7, stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n","        self.bn1 = nn.BatchNorm3d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)\n","        self.layer2 = self._make_layer(\n","            block, 64*2, layers[1], shortcut_type, stride=2)\n","        self.layer3 = self._make_layer(\n","            block, 128*2, layers[2], shortcut_type, stride=1, dilation=2)\n","        self.layer4 = self._make_layer(\n","            block, 256*2, layers[3], shortcut_type, stride=1, dilation=4)\n","\n","        self.fea_dim = 256*2 * block.expansion\n","        self.fc = nn.Sequential(nn.Linear(self.fea_dim, num_class, bias=True))\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv3d):\n","                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')\n","            elif isinstance(m, nn.BatchNorm3d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1, dilation=1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","\n","            if shortcut_type == 'A':\n","                downsample = partial(\n","                    downsample_basic_block,\n","                    planes=planes * block.expansion,\n","                    stride=stride,\n","                    no_cuda=self.no_cuda)\n","            else:\n","                downsample = nn.Sequential(\n","                    nn.Conv3d(\n","                        self.inplanes,\n","                        planes * block.expansion,\n","                        kernel_size=1,\n","                        stride=stride,\n","                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride=stride, dilation=dilation, downsample=downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, dilation=dilation))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.conv1( x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = F.adaptive_avg_pool3d(x, (1, 1, 1))\n","        emb_3d = x.view((-1, self.fea_dim))\n","        out = self.fc(emb_3d)\n","        return out\n","\n","\n","def resnet10(**kwargs):\n","    \"\"\"Constructs a ResNet-18 model.\n","    \"\"\"\n","    model = ResNet3D(BasicBlock, [1, 1, 1, 1],**kwargs)\n","    return model\n","\n","def resnet3d_10(**kwargs):\n","    \"\"\"Constructs a ResNet-18 model.\n","    \"\"\"\n","    model = ResNet3D(BasicBlock, [1, 1, 1, 1], **kwargs)\n","    return model\n","\n","def resnet18(**kwargs):\n","    \"\"\"Constructs a ResNet-18 model.\n","    \"\"\"\n","    model = ResNet3D(BasicBlock, [2, 2, 2, 2], **kwargs)\n","    return model\n","\n","def resnet34(**kwargs):\n","    \"\"\"Constructs a ResNet-34 model.\n","    \"\"\"\n","    model = ResNet3D(BasicBlock, [3, 4, 6, 3], **kwargs)\n","    return model\n","\n","def resnet50(**kwargs):\n","    \"\"\"Constructs a ResNet-50 model.\n","    \"\"\"\n","    model = ResNet3D(Bottleneck, [3, 4, 6, 3], **kwargs)\n","    return model\n","\n","def resnet101(**kwargs):\n","    \"\"\"Constructs a ResNet-101 model.\n","    \"\"\"\n","    model = ResNet3D(Bottleneck, [3, 4, 23, 3], **kwargs)\n","    return model\n","\n","def resnet152(**kwargs):\n","    \"\"\"Constructs a ResNet-101 model.\n","    \"\"\"\n","    model = ResNet3D(Bottleneck, [3, 8, 36, 3], **kwargs)\n","    return model\n","\n","def resnet200(**kwargs):\n","    \"\"\"Constructs a ResNet-101 model.\n","    \"\"\"\n","    model = ResNet3D(Bottleneck, [3, 24, 36, 3], **kwargs)\n","    return model\n"],"execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["class TReNDSModel(nn.Module):\n","    def __init__(self):\n","        super(TReNDSModel, self).__init__()\n","        \n","        # modules = list(resnet50().children())[:-1]\n","        modules = list(resnet10().children())[:-1]\n","        self.resnet = nn.Sequential(*modules)\n","        \n","        self.m1 = nn.MaxPool3d(kernel_size=(3, 3, 3))\n","        self.f0 = nn.Flatten()\n","        self.l0 = nn.Linear(5500, 1024)\n","        # self.l0 = nn.Linear(17788, 1024) # resnet10 -> 4096, resnet50 -> 16384\n","        self.p0 = nn.PReLU()\n","        self.l1 = nn.Linear(1024, 256)\n","        self.p1 = nn.PReLU()\n","        self.l2 = nn.Linear(256, 2)\n","        self.sigmoid = nn.Sigmoid()\n","        \n","        \n","    def forward(self, inputs, fnc, loading):\n","        features = self.resnet(inputs)\n","        x = self.m1(features)\n","        flatten = self.f0(x) #shape=(batch, 16384) +(batch, 1378)) + (bathc, 26)\n","        x = torch.cat([flatten, fnc, loading], dim=1) #shape(batch, 16384+1378+26)\n","        x = self.l0(x)\n","        x = self.p0(x)\n","        x = self.l1(x)\n","        x = self.p1(x)\n","        out = self.l2(x)\n","        out = self.sigmoid(out)\n","        return out"],"execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":12,"metadata":{"tags":[]},"outputs":[],"source":["# m = nn.Sigmoid()\n","# loss = nn.BCELoss()\n","# input = torch.randn(3, requires_grad=True)\n","# target = torch.empty(3).random_(2)\n","# print(input.shape, target.shape)\n","# print(input, target)\n","# output = loss(m(input), target)\n","# print(output)\n","# output.backward()\n","# print(output)"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# def count_parameters(model):\n","#     params = []\n","#     for p in model.parameters():\n","#         params.append(p.numel()) \n","#     return params\n","\n","# def count_trainable_parameters(model):\n","#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","\n","model = TReNDSModel()\n","model\n","\n","\n","# num_parameters=count_parameters(model)\n","# print(num_parameters)\n","# num_parameters=count_trainable_parameters(model)\n","# print(num_parameters)"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":"TReNDSModel(\n  (resnet): Sequential(\n    (0): Conv3d(53, 64, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(2, 2, 2), dilation=(2, 2, 2), bias=False)\n        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(2, 2, 2), dilation=(2, 2, 2), bias=False)\n        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(4, 4, 4), dilation=(4, 4, 4), bias=False)\n        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(4, 4, 4), dilation=(4, 4, 4), bias=False)\n        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n  )\n  (m1): MaxPool3d(kernel_size=(3, 3, 3), stride=(3, 3, 3), padding=0, dilation=1, ceil_mode=False)\n  (f0): Flatten()\n  (l0): Linear(in_features=5500, out_features=1024, bias=True)\n  (p0): PReLU(num_parameters=1)\n  (l1): Linear(in_features=1024, out_features=256, bias=True)\n  (p1): PReLU(num_parameters=1)\n  (l2): Linear(in_features=256, out_features=2, bias=True)\n  (sigmoid): Sigmoid()\n)"},"metadata":{},"execution_count":15}]},{"metadata":{},"cell_type":"markdown","source":["# Dataset"]},{"metadata":{"trusted":true},"cell_type":"code","source":["class MRIMapDataset(Dataset):\n","    def __init__(self, df=None, fnc=None, loading=None, mode=\"train\"):\n","        super(Dataset, self).__init__()\n","        self.mode = mode\n","        self.fnc = fnc.iloc[:, 1:-2].values\n","        self.loading = loading.iloc[:, 1:-2].values\n","        \n","        if mode == \"train\":\n","            # self.labels = df[['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']].values\n","            self.labels = df[\"is_site2\"].values\n","            self.list_IDs = df[\"Id\"].values.astype(str)\n","        elif mode == \"test\":\n","            list1 = os.listdir(config.root_test_path)\n","            self.list_IDs = sorted(list1)\n","\n","    def __len__(self):\n","        return len(self.list_IDs)\n","    \n","    def __getitem__(self, idx):\n","        if self.mode == \"train\":\n","            scan_id = self.list_IDs[idx]        \n","            subject_filename = config.root_train_path + '/' + scan_id + '.mat'\n","            subject_data = h5py.File(subject_filename, 'r')['SM_feature'][()]\n","            subject_data = np.moveaxis(subject_data, [0, 1, 2, 3], [3, 2, 1, 0])\n","            fnc = self.fnc[idx] / 600.0\n","            loading = self.loading[idx]\n","            return {\n","                'scan_maps': torch.tensor(subject_data, dtype=torch.float),\n","                'fnc': torch.tensor(fnc, dtype=torch.float),\n","                'loading': torch.tensor(loading, dtype=torch.float),\n","                'targets': torch.tensor(self.labels[idx, ], dtype=torch.int)\n","            }\n","        elif self.mode == \"test\":\n","            scan_id = self.list_IDs[idx]        \n","            subject_filename = config.root_test_path + '/' + scan_id\n","            subject_data = h5py.File(subject_filename, 'r')['SM_feature'][()]\n","            subject_data = np.moveaxis(subject_data, [0, 1, 2, 3], [3, 2, 1, 0])\n","            fnc = self.fnc[idx].values / 600.0\n","            loading = self.loading[idx].values\n","            return {\n","                'scan_maps': torch.tensor(subject_data, dtype=torch.float),\n","                'fnc': torch.tensor(fnc, dtype=torch.float),\n","                'loading': torch.tensor(loading, dtype=torch.float),\n","            }"],"execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## Early Stopping"]},{"metadata":{"trusted":true},"cell_type":"code","source":["class EarlyStopping:\n","    def __init__(self, patience=5, checkpoint_path='checkpoint.pth', device=\"cpu\"):\n","        self.patience = patience\n","        self.checkpoint_path = checkpoint_path\n","        self.counter = 0\n","        self.best_score = None\n","        self.device = device\n","\n","    def load_best_weights(self, model):\n","        model.load_state_dict(torch.load(self.checkpoint_path, map_location=self.device))\n","\n","    def __call__(self, score, model, mode=\"min\"):\n","        # cpuでも使用できるようにするためにパラメータを一度cpuに変換してから保存し、再度deviceに直す\n","        if mode == \"max\":\n","            if self.best_score is None or (score > self.best_score):\n","                torch.save(model.to('cpu').state_dict(), self.checkpoint_path)\n","                model.to(self.device)\n","                self.best_score, self.counter = score, 0\n","                return 1\n","            else:\n","                self.counter += 1\n","                if self.counter >= self.patience:\n","                    return 2\n","        elif mode == \"min\":\n","            if self.best_score is None or (score < self.best_score):\n","                torch.save(model.to('cpu').state_dict(), self.checkpoint_path)\n","                model.to(self.device)\n","                self.best_score, self.counter = score, 0\n","                return 1\n","            else:\n","                self.counter += 1\n","                if self.counter >= self.patience:\n","                    return 2\n","        return 0\n"],"execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# GPU Fitter"]},{"metadata":{"trusted":true},"cell_type":"code","source":["class GPUFitter:\n","    def __init__(self, model, fold, device, config, save_model_path=\"checkpoint.pth\", log_path=\"log.csv\"):\n","        self.model = model\n","        self.device = device\n","        self.log_path = log_path[:-4] +f\"_fold{fold}_No{file_No}.csv\"\n","        self.save_model_path = save_model_path[:-4] +f\"_fold{fold}_No{file_No}.pth\"\n","        \n","        self.epoch = 0\n","        self.fold = fold\n","        self.config = config\n","                \n","        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.learning_rate)\n","        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","            self.optimizer, \n","            mode='min', \n","            patience=3, \n","            factor=0.3, \n","            verbose=True\n","        )\n","        self.early_stopping = EarlyStopping(patience=10, device=device, checkpoint_path=self.save_model_path)\n","        # logits = F.softmax(torch.rand(10,no_of_classes)).float()#.view(-1)\n","        # labels = torch.empty(10).random_(2).to(torch.int64)\n","        self.no_of_classes = 2\n","        self.beta = 0.9999\n","        self.gamma = 2.0\n","        self.samples_per_cls = [1176 * 4, 102 * 4]\n","        self.samples_per_cls_valid = [1176 * 4, 102 * 1]\n","        self.loss_type = \"focal\"\n","        # cb_loss = CB_loss(labels, logits, samples_per_cls, no_of_classes,loss_type, beta, gamma)\n","        self.criterion = CB_loss  # TReNDSLoss(self.device)\n","                \n","        self.log(f'Fitter prepared for fold {self.fold}. Device is {self.device} target:{config.target[target_id]}')\n","        self.columns = [\"loss\", \"score\", \"val_loss\", \"val_score\", \"lr\"]\n","        self.log_df = pd.DataFrame(columns=self.columns)\n","        \n","    def fit(self, train_loader, valid_loader):\n","        for e in range(self.config.epochs):\n","            lr = self.optimizer.param_groups[0]['lr']\n","            if self.config.verbose:\n","                timestamp = datetime.utcnow().isoformat()\n","                self.log(f'\\n{timestamp}\\nLR:{lr:.5f}')\n","            \n","            t = time.time()\n","            loss, score = self.train_one_epoch(train_loader)\n","            val_loss, val_score = self.validation_one_epoch(valid_loader)\n","            print(f'Epoch: {self.epoch}, loss: {loss.avg:.5f}, score: {score:.5f},'\\\n","                  f'val_loss: {val_loss.avg:.5f}, val_score: {val_score:.5f},'\\\n","                  f'time:{(time.time() - t):.5f}, lr{lr:.7f}' )\n","            res = self.early_stopping(val_score, self.model, mode=\"max\")\n","            self.scheduler.step(val_loss.avg)\n","            tmp = pd.DataFrame([[loss.avg, score, val_loss.avg, val_score, lr]], columns=self.columns)\n","            self.log_df = pd.concat([self.log_df, tmp], axis=0)\n","            self.log_df.to_csv(self.log_path, index=False)\n","            if res == 2:\n","                print(\"Early Stopping\")\n","                print(self.log_path)\n","                print(self.save_model_path)\n","                break\n","            self.epoch += 1\n","    \n","    def train_one_epoch(self, train_loader):\n","        self.model.train()\n","        losses = AverageMeter()\n","        t = time.time()\n","        _targets = []\n","        _outputs = []\n","        for step, data in enumerate(train_loader):\n","            scan_maps = data['scan_maps']\n","            fnc = data['fnc']\n","            loading =data['loading']\n","            targets = data['targets']\n","            \n","            scan_maps = scan_maps.to(self.device, dtype=torch.float)\n","            fnc = fnc.to(self.device, dtype=torch.float)\n","            loading = loading.to(self.device, dtype=torch.float)\n","            targets = targets.to(self.device, dtype=torch.int)\n","            self.optimizer.zero_grad()\n","            \n","            outputs = self.model(scan_maps, fnc, loading)\n","            loss = self.criterion(targets, outputs, self.samples_per_cls, self.no_of_classes, self.loss_type, self.beta, self.gamma)\n","            \n","            batch_size = scan_maps.size(0)\n","            losses.update(loss.detach().item(), batch_size)\n","            \n","            targets = targets.detach().cpu().numpy()\n","            outputs = outputs.detach().cpu().numpy()\n","            _targets += list(targets)\n","            _outputs += list(outputs)\n","            loss.backward()\n","            self.optimizer.step()\n","            if self.config.verbose:\n","                if step % self.config.verbose_step == 0:\n","                    self.log(\n","                        f'Train Step {step}, ' + \\\n","                        f'fold {self.fold}, ' + \\\n","                        f'loss: {losses.avg:.5f}, ' + \\\n","                        f'time: {(time.time() - t):.5f}'\n","                    )\n","        scores = f1_score(_targets, _outputs) \n","        return losses, scores\n","    \n","    def validation_one_epoch(self, validation_loader):\n","        self.model.eval()\n","        \n","        losses = AverageMeter()\n","        t = time.time()\n","        _targets = []\n","        _outputs = []\n","        with torch.no_grad():\n","            for step, data in enumerate(validation_loader):\n","                scan_maps = data['scan_maps']\n","                fnc = data['fnc']\n","                loading =data['loading']\n","                targets = data['targets']\n","\n","                scan_maps = scan_maps.to(self.device, dtype=torch.float)\n","                fnc = fnc.to(self.device, dtype=torch.float)\n","                loading = loading.to(self.device, dtype=torch.float)\n","                targets = targets.to(self.device, dtype=torch.float)\n","                outputs = self.model(scan_maps, fnc, loading)\n","\n","                loss = self.criterion(targets, outputs, self.samples_per_cls_valid, self.no_of_classes, self.loss_type, self.beta, self.gamma)\n","                batch_size = scan_maps.size(0)\n","                losses.update(loss.detach().item(), batch_size)\n","                \n","                targets = targets.detach().cpu().numpy()\n","                outputs = outputs.detach().cpu().numpy()\n","                _targets += list(targets)\n","                _outputs += list(outputs)\n","                if self.config.verbose:\n","                    if step % self.config.verbose_step == 0:\n","                        self.log(\n","                        f'Validation Step {step}, ' + \\\n","                        f'fold {self.fold}, ' + \\\n","                        f'loss: {losses.avg:.5f}, ' + \\\n","                        # f'competition metric: {scores.avg:.5f}, ' + \\\n","                        f'time: {(time.time() - t):.5f}'\n","                        )\n","        scores = f1_score(_targets, _outputs)\n","        return losses, scores\n","    \n","    def log(self, message):\n","        if self.config.verbose:\n","            print(message)"],"execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Loading data"]},{"metadata":{},"cell_type":"markdown","source":["## num_workerについて\n","\n","0だとメインプロセスのみがバッチをロードして、1以上だとサブプロセスが生えて代わりにロードしてくれるらしい  \n","これを1以上にすると、Pythonのコードを実行してるメインプロセスとは別のワーカープロセスがメインプロセスと並列的にデータのロードを行ってメモリにキューして行ってくれるので、メインプロセスは、データのロード以外の仕事に集中できる。  \n","ただし、ワーカープロセス数は増やせばいいってもんじゃなくて、メインプロセスの他の処理の忙しさとか、CPUコア数とかバッチサイズとかにも複雑に依存するので、実測値がデフォルトよりもよくなるかはわからん。  \n","ワーカープロセスが過多だと、メインプロセスが次のバッチを必要とするまでにメモリが詰まったり、その分CPUが占領され流とか、メモリが足りなくなるとか  \n","\n","#### 例\n","すべてのデータを使うと (num_worker=8, batch_size=16)のときはメモリエラー  \n","すべてのデータを使うと (num_worker=4, batch_size=16)のときはうまくいく  \n","https://deeplizard.com/learn/video/kWVgvsejXsE これがnum_workerについて分かりやすい\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"tags":[]},"outputs":[],"source":["# met = []\n","# a = np.array([1,2,3])\n","# b = np.array([2,3,4])\n","# met += list(a >= 2)\n","# print(met)\n","# met += list(b >= 2)\n","# print(met)\n","# target = [0, 1, 1, 0, 1, 1]\n","# s = f1_score(target, met)\n","# print(s)\n"]},{"metadata":{"trusted":true},"cell_type":"code","source":["train_df = pd.read_csv(\"../input/trends-assessment-prediction/train_scores.csv\")\n","drop_cols = [\"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]\n","train_df.drop(drop_cols, axis=1, inplace=True)\n","train_df[\"is_train\"] = True\n","# train_df[\"kfold\"] = df[\"kfold\"].astype(int)\n","\n","fnc = pd.read_csv(\"../input/trends-assessment-prediction/fnc.csv\")\n","fnc.fillna(fnc.mean(), inplace=True)\n","fnc = fnc.merge(train_df, on=\"Id\", how=\"left\")\n","test_fnc = fnc[fnc[\"is_train\"] != True].copy()\n","fnc = fnc[fnc[\"is_train\"] == True].copy()\n","\n","loading = pd.read_csv(\"../input/trends-assessment-prediction/loading.csv\")\n","loading.fillna(loading.mean(), inplace=True)\n","loading = loading.merge(train_df, on=\"Id\", how=\"left\")\n","test_loading = loading[loading[\"is_train\"] != True].copy()\n","loading = loading[loading[\"is_train\"] == True].copy()"],"execution_count":20,"outputs":[]},{"cell_type":"code","execution_count":21,"metadata":{"tags":[]},"outputs":[],"source":["# devide test data site2 and unknow\n","site2_id = pd.read_csv(\"../input/trends-assessment-prediction/reveal_ID_site2.csv\")\n","site2_id['is_site2'] = 1\n","loading[\"is_site2\"] = 0\n","fnc[\"is_site2\"] = 0\n","test_loading = pd.merge(test_loading, site2_id, how=\"left\")\n","test_loading.loc[test_loading[\"is_site2\"] != 1, [\"is_site2\"]] = \"unknow\"\n","test_fnc = pd.merge(test_fnc, site2_id, how=\"left\")\n","test_fnc.loc[test_fnc[\"is_site2\"] != 1, [\"is_site2\"]] = \"unknow\"\n","\n","adversal_loading_df = pd.concat([loading, test_loading[test_loading[\"is_site2\"] == 1]], axis=0).reset_index(drop=True).drop(\"is_train\", axis=1)\n","adversal_fnc_df = pd.concat([fnc, test_fnc[test_fnc[\"is_site2\"] == 1]], axis=0).reset_index(drop=True).drop(\"is_train\", axis=1)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["adversal_loading_df['kfold'] = -1\n","adversal_fnc_df['kfold'] = -1\n","\n","kf = StratifiedKFold(n_splits=config.num_folds, shuffle=True, random_state=config.seed)\n","for fold, (trn_, val_) in enumerate(kf.split(adversal_loading_df[adversal_loading_df.columns[1:-1]], adversal_loading_df[\"is_site2\"].astype(int))):\n","    adversal_loading_df.loc[val_, 'kfold'] = fold\n","    adversal_fnc_df.loc[val_, 'kfold'] = fold\n","adversal_target_df = adversal_loading_df[[\"Id\", \"is_site2\", \"kfold\"]]"]},{"metadata":{},"cell_type":"markdown","source":["# Running on multiple folds"]},{"metadata":{"trusted":true,"tags":[]},"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","save_model_path = \"adversal_resnet10.pth\"\n","log_path = \"log_adversal_resnet10.csv\"\n","print(device)\n","print(save_model_path)\n","print(log_path)"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":"cuda\nadversal_resnet10.pth\nlog_adversal_resnet10.csv\n"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["def run(fold):\n","    \n","    model = TReNDSModel()\n","    model.to(device)\n","    \n","    adversal_train_loading_df = adversal_loading_df[adversal_loading_df['kfold'] != fold].reset_index(drop=True)\n","    adversal_valid_loading_df = adversal_loading_df[adversal_loading_df['kfold'] == fold].reset_index(drop=True)\n","    adversal_train_fnc_df = adversal_fnc_df[adversal_fnc_df['kfold'] != fold].reset_index(drop=True)\n","    adversal_valid_fnc_df = adversal_fnc_df[adversal_fnc_df['kfold'] == fold].reset_index(drop=True)\n","    adversal_target_train_df = adversal_target_df[adversal_target_df['kfold'] != fold].reset_index(drop=True)\n","    adversal_target_valid_df = adversal_target_df[adversal_target_df['kfold'] != fold].reset_index(drop=True)\n","    \n","    train_dataset = MRIMapDataset(df=adversal_target_train_df, fnc=adversal_train_fnc_df, loading=adversal_train_loading_df, mode=\"train\")\n","    valid_dataset = MRIMapDataset(df=adversal_target_valid_df, fnc=adversal_valid_fnc_df, loading=adversal_valid_loading_df, mode=\"train\")\n","    \n","    train_data_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=config.batch_size,\n","        num_workers=config.num_workers,\n","        shuffle=True\n","    )\n","    \n","    valid_data_loader = torch.utils.data.DataLoader(\n","        valid_dataset,\n","        batch_size=config.batch_size,\n","        num_workers=config.num_workers,\n","        shuffle=False\n","    )\n","    \n","    fitter = GPUFitter(model, fold, device, config, save_model_path=save_model_path, log_path=log_path)\n","    fitter.fit(train_data_loader, valid_data_loader)\n","    print('over')\n","    return fitter"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["fitter = run(config.fold)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["log_df = pd.read_csv(fitter.log_path)\n","# log_df = pd.read_csv(\"log0.csv\")\n","# log_df = pd.read_csv('../input/trend3dcnn/log0.csv')\n","\n","log_df"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["ptcture_path = log_path[:-4] +f\"_fold{config.fold}_No{file_No}.png\"\n","plt.figure(figsize=(15,5))\n","plt.title(\"loss\")\n","plt.subplot(1,2,1)\n","log_df.loss.plot()\n","log_df.val_loss.plot()\n","plt.subplot(1,2,2)\n","plt.title(\"score\")\n","log_df.score.plot()\n","log_df.val_score.plot()\n","plt.savefig(f\"adversal/{ptcture_path}\")"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# train_df = df[df['kfold'] != fold].reset_index(drop=True)\n","# train_dataset = MRIMapDataset(df=train_df, mode=\"train\")\n","# train_dataset[0][\"scan_maps\"].shape"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Parallel(n_jobs=config.num_folds, backend=\"threading\")(delayed(run)(i) for i in range(config.num_folds))"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# valid data and test data "]},{"metadata":{"trusted":true},"cell_type":"code","source":["# resnet10でだいたい45分 num_worker=0のとき\n","adversal_valid_fnc_df = adversal_fnc_df[adversal_fnc_df['kfold'] == config.fold].reset_index(drop=True)\n","adversal_valid_loading_df = adversal_target_df[adversal_loading_df['kfold'] == config.fold].reset_index(drop=True)\n","\n","valid_dataset = MRIMapDataset(fnc=adversal_valid_fnc_df, loading=adversal_valid_loading_df, mode=\"test\")\n","valid_data_loader = torch.utils.data.DataLoader(\n","    valid_dataset,\n","    batch_size=config.batch_size,\n","    num_workers=config.num_workers,\n","    shuffle=False)\n","_test_loading = test_loading.loc[test_loading[\"is_site2\"] != 1]\n","_test_fnc = test_fnc.loc[test_fnc[\"is_site2\"] != 1]\n","test_dataset = MRIMapDataset(fnc=_test_fnc, loading=_test_loading, mode=\"test\")\n","test_dataloader = torch.utils.data.DataLoader(\n","        test_dataset,\n","        batch_size=config.batch_size,\n","        num_workers=config.num_workers,\n","        shuffle=False)\n","model = TReNDSModel()\n","model.load_state_dict(torch.load(fitter.save_model_path))#'../input/trend3dcnn/checkpoint0.pth'\n","model.to(device)\n","model.eval()\n","\n","test_preds = np.empty((0, 2))\n","valid_preds = np.empty((0, 2))\n","with torch.no_grad():\n","    for step, data in enumerate(tqdm(test_dataloader)):\n","        scan_maps = data['scan_maps']\n","        fnc = data['fnc']\n","        loading =data['loading']       \n","        scan_maps = scan_maps.to(device, dtype=torch.float)\n","        fnc = fnc.to(device, dtype=torch.float)\n","        loading = loading.to(device, dtype=torch.float)\n","        \n","        outputs = model(scan_maps, fnc, loading)\n","        batch_size = scan_maps.size(0)\n","        outputs = outputs.detach().cpu().numpy()\n","        test_preds = np.concatenate([test_preds, outputs], 0)\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","    test_df = pd.DataFrame(test_preds, columns=[\"site1\", \"site2\"]])\n","    test_df.to_csv(f\"adversal/test_fold{config.fold}_No{file_No}.csv\", index=False)\n","    for step, data in enumerate(tqdm(valid_data_loader)):\n","        scan_maps = data['scan_maps']\n","        fnc = data['fnc']\n","        loading =data['loading']       \n","        scan_maps = scan_maps.to(device, dtype=torch.float)\n","        fnc = fnc.to(device, dtype=torch.float)\n","        loading = loading.to(device, dtype=torch.float)\n","        \n","        outputs = model(scan_maps, fnc, loading)\n","        batch_size = scan_maps.size(0)\n","        outputs = outputs.detach().cpu().numpy()\n","        valid_preds = np.concatenate([valid_preds, outputs], 0)\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","    valid_df = pd.DataFrame(valid_preds, columns=[\"site1\", \"site2\"]])\n","    valid_df.to_csv(f\"adversal/valid_fold{config.fold}_No{file_No}.csv\", index=False)\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["print(outputs.shape)\n","print(test_preds)\n","print(valid_preds.shape)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# test_df = pd.DataFrame(test_preds, columns=[\"age\", \"domain1_var1\", \"domain1_var2\",\"domain2_var1\", \"domain2_var2\"])\n","# test_df = pd.DataFrame(test_preds, columns=[\"site1\", \"site2\"]])\n","# test_df.describe()\n","# test_df.to_csv(f\"test_fold{config.fold}_No{file_No}.csv\", index=False)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# list1 = os.listdir(config.root_test_path)\n","# list2 = sorted(list1)\n","# test_df[\"Id\"] = list2\n","# test_df[\"Id\"] = test_df[\"Id\"].map(lambda x: x[:-4])\n","# test_df.set_index(\"Id\", drop=True, inplace=True)\n","# test_df"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# df_long = test_df.stack().reset_index()\n","# df_long.rename(columns={'level_1': 'target', 0: 'Predicted'}, inplace=True)\n","# df_long[\"Id\"] = df_long[\"Id\"] + \"_\" + df_long[\"target\"]\n","# df_long.drop(\"target\", axis=1, inplace=True)\n","# df_long.to_csv('submission_No{file_No}.csv', index=False)"],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.8-final","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}