{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":["file_No = 30\n","fold = 0"],"execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Importing dependencies"]},{"metadata":{"trusted":true},"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset\n","from sklearn.metrics import f1_score\n","import numpy as np\n","import pandas as pd\n","\n","import random\n","from tqdm.notebook import tqdm\n","import math\n","from functools import partial\n","import h5py\n","from datetime import datetime\n","import os\n","import time\n","import gc\n","from joblib import Parallel, delayed\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from sklearn.decomposition import PCA\n","import nilearn as nl\n","import nilearn.plotting as nlplt\n","import nibabel as nib"],"execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Config"]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":["class config:\n","    epochs = 100\n","    batch_size = 16\n","    test_batch_size = 16\n","    learning_rate = 1e-3\n","    fMRI_mask_path = '../input/trends-assessment-prediction/fMRI_mask.nii'\n","    root_train_path = '../input/trends-assessment-prediction/fMRI_train'\n","    root_test_path = '../input/trends-assessment-prediction/fMRI_test'\n","    num_folds = 5\n","    seed = 2020\n","    verbose = True\n","    verbose_step = 1\n","    num_workers = 4\n","    test_num_workers = 4\n","    target = [\"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]\n","    weight = [0.3, 0.175, 0.175, 0.175, 0.175]\n","    # cross validationをするときはここでfoldを変更する\n","    fold = fold"],"execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"tags":[]},"cell_type":"code","source":["print(\"fold\",config.fold, \"file_No\", file_No)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":"fold 0 file_No 30\n"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","seed_everything(config.seed)"],"execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Metrics"]},{"metadata":{"trusted":true},"cell_type":"code","source":["class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","    \n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","    \n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Loss function"]},{"metadata":{"trusted":true,"tags":[]},"cell_type":"code","source":["def focal_loss(labels, logits, alpha, gamma):\n","    \"\"\"Compute the focal loss between `logits` and the ground truth `labels`.\n","    Focal loss = -alpha_t * (1-pt)^gamma * log(pt)\n","    where pt is the probability of being classified to the true class.\n","    pt = p (if true class), otherwise pt = 1 - p. p = sigmoid(logit).\n","    Args:\n","      labels: A float tensor of size [batch, num_classes].\n","      logits: A float tensor of size [batch, num_classes].\n","      alpha: A float tensor of size [batch_size]\n","        specifying per-example weight for balanced cross entropy.\n","      gamma: A float scalar modulating loss from hard and easy examples.\n","    Returns:\n","      focal_loss: A float32 scalar representing normalized total loss.\n","    \"\"\"    \n","    BCLoss = F.binary_cross_entropy_with_logits(input = logits, target = labels,reduction = \"none\")\n","\n","    if gamma == 0.0:\n","        modulator = 1.0\n","    else:\n","        modulator = torch.exp(-gamma * labels * logits - gamma * torch.log(1 + \n","            torch.exp(-1.0 * logits)))\n","\n","    loss = modulator * BCLoss\n","\n","    weighted_loss = alpha * loss\n","    focal_loss = torch.sum(weighted_loss)\n","\n","    focal_loss /= torch.sum(labels)\n","    return focal_loss\n","\n","\n","\n","def CB_loss(labels, logits, samples_per_cls, no_of_classes, loss_type, beta, gamma):\n","    \"\"\"Compute the Class Balanced Loss between `logits` and the ground truth `labels`.\n","    Class Balanced Loss: ((1-beta)/(1-beta^n))*Loss(labels, logits)\n","    where Loss is one of the standard losses used for Neural Networks.\n","    Args:\n","      labels: A int tensor of size [batch].\n","      logits: A float tensor of size [batch, no_of_classes].\n","      samples_per_cls: A python list of size [no_of_classes].\n","      no_of_classes: total number of classes. int\n","      loss_type: string. One of \"sigmoid\", \"focal\", \"softmax\".\n","      beta: float. Hyperparameter for Class balanced loss.\n","      gamma: float. Hyperparameter for Focal loss.\n","    Returns:\n","      cb_loss: A float tensor representing class balanced loss\n","    \"\"\"\n","    effective_num = 1.0 - np.power(beta, samples_per_cls)\n","    weights = (1.0 - beta) / np.array(effective_num)\n","    weights = weights / np.sum(weights) * no_of_classes\n","\n","    labels_one_hot = F.one_hot(labels, no_of_classes).float()\n","\n","    weights = torch.tensor(weights).float().to(\"cpu\")\n","    weights = weights.unsqueeze(0)\n","    weights = weights.repeat(labels_one_hot.shape[0],1) * labels_one_hot\n","    weights = weights.sum(1)\n","    weights = weights.unsqueeze(1)\n","    weights = weights.repeat(1,no_of_classes)\n","\n","    if loss_type == \"focal\":\n","        cb_loss = focal_loss(labels_one_hot, logits, weights, gamma)\n","    elif loss_type == \"sigmoid\":\n","        cb_loss = F.binary_cross_entropy_with_logits(input = logits,target = labels_one_hot, weight = weights)\n","    elif loss_type == \"softmax\":\n","        pred = logits.softmax(dim = 1)\n","        cb_loss = F.binary_cross_entropy(input = pred, target = labels_one_hot, weight = weights)\n","    return cb_loss\n","no_of_classes = 2\n","logits = F.softmax(torch.rand(10,no_of_classes)).float()#.view(-1)\n","labels = torch.empty(10).random_(2).to(torch.int64)\n","print(logits)\n","print(labels)\n","beta = 0.9999\n","gamma = 1.0\n","samples_per_cls = [400, 4000]\n","loss_type = \"focal\"\n","cb_loss = CB_loss(labels, logits, samples_per_cls, no_of_classes,loss_type, beta, gamma)\n","print(cb_loss)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":"tensor([[0.5943, 0.4057],\n        [0.6159, 0.3841],\n        [0.5529, 0.4471],\n        [0.4451, 0.5549],\n        [0.4062, 0.5938],\n        [0.4346, 0.5654],\n        [0.5699, 0.4301],\n        [0.5822, 0.4178],\n        [0.6635, 0.3365],\n        [0.3180, 0.6820]])\ntensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0])\ntensor(1.1445)\n"}]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"tensor(1.1445)"},"metadata":{},"execution_count":9}],"source":["c = CB_loss\n","c(labels, logits, samples_per_cls, no_of_classes,loss_type, beta, gamma)"]},{"metadata":{},"cell_type":"markdown","source":["# Model"]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":["__all__ = [\n","    'resnet10', \n","    'resnet18', \n","    'resnet34', \n","    'resnet50', \n","    'resnet101',\n","    'resnet152', \n","    'resnet200'\n","]\n","\n","def conv3x3x3(in_planes, out_planes, stride=1, dilation=1):\n","    # 3x3x3 convolution with padding\n","    return nn.Conv3d(\n","        in_planes,\n","        out_planes,\n","        kernel_size=3,\n","        dilation=dilation,\n","        stride=stride,\n","        padding=dilation,\n","        bias=False)\n","\n","def downsample_basic_block(x, planes, stride, no_cuda=False):\n","    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n","    zero_pads = torch.Tensor(out.size(0), planes - out.size(1), out.size(2), out.size(3), out.size(4)).zero_()\n","    if not no_cuda:\n","        if isinstance(out.data, torch.cuda.FloatTensor):\n","            zero_pads = zero_pads.cuda()\n","\n","    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n","    return out\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3x3(inplanes, planes, stride=stride, dilation=dilation)\n","        self.bn1 = nn.BatchNorm3d(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3x3(planes, planes, dilation=dilation)\n","        self.bn2 = nn.BatchNorm3d(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.dilation = dilation\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm3d(planes)\n","        self.conv2 = nn.Conv3d(\n","            planes, planes, kernel_size=3, stride=stride, dilation=dilation, padding=dilation, bias=False)\n","        self.bn2 = nn.BatchNorm3d(planes)\n","        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm3d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.dilation = dilation\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","        return out\n","\n","class ResNet3D(nn.Module):\n","\n","    def __init__(self,\n","                 block,\n","                 layers,\n","                 shortcut_type='B',\n","                 num_class = 5,\n","                 no_cuda=False):\n","\n","        self.inplanes = 64\n","        self.no_cuda = no_cuda\n","        super(ResNet3D, self).__init__()\n","\n","        # 3D conv net\n","        self.conv1 = nn.Conv3d(53, 64, kernel_size=7, stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n","        # self.conv1 = nn.Conv3d(1, 64, kernel_size=7, stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n","        self.bn1 = nn.BatchNorm3d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)\n","        self.layer2 = self._make_layer(\n","            block, 64*2, layers[1], shortcut_type, stride=2)\n","        self.layer3 = self._make_layer(\n","            block, 128*2, layers[2], shortcut_type, stride=1, dilation=2)\n","        self.layer4 = self._make_layer(\n","            block, 256*2, layers[3], shortcut_type, stride=1, dilation=4)\n","\n","        self.fea_dim = 256*2 * block.expansion\n","        self.fc = nn.Sequential(nn.Linear(self.fea_dim, num_class, bias=True))\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv3d):\n","                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')\n","            elif isinstance(m, nn.BatchNorm3d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1, dilation=1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","\n","            if shortcut_type == 'A':\n","                downsample = partial(\n","                    downsample_basic_block,\n","                    planes=planes * block.expansion,\n","                    stride=stride,\n","                    no_cuda=self.no_cuda)\n","            else:\n","                downsample = nn.Sequential(\n","                    nn.Conv3d(\n","                        self.inplanes,\n","                        planes * block.expansion,\n","                        kernel_size=1,\n","                        stride=stride,\n","                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride=stride, dilation=dilation, downsample=downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, dilation=dilation))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.conv1( x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = F.adaptive_avg_pool3d(x, (1, 1, 1))\n","        emb_3d = x.view((-1, self.fea_dim))\n","        out = self.fc(emb_3d)\n","        return out\n","\n","\n","def resnet10(**kwargs):\n","    \"\"\"Constructs a ResNet-18 model.\n","    \"\"\"\n","    model = ResNet3D(BasicBlock, [1, 1, 1, 1],**kwargs)\n","    return model\n","\n","def resnet3d_10(**kwargs):\n","    \"\"\"Constructs a ResNet-18 model.\n","    \"\"\"\n","    model = ResNet3D(BasicBlock, [1, 1, 1, 1], **kwargs)\n","    return model\n","\n","def resnet18(**kwargs):\n","    \"\"\"Constructs a ResNet-18 model.\n","    \"\"\"\n","    model = ResNet3D(BasicBlock, [2, 2, 2, 2], **kwargs)\n","    return model\n","\n","def resnet34(**kwargs):\n","    \"\"\"Constructs a ResNet-34 model.\n","    \"\"\"\n","    model = ResNet3D(BasicBlock, [3, 4, 6, 3], **kwargs)\n","    return model\n","\n","def resnet50(**kwargs):\n","    \"\"\"Constructs a ResNet-50 model.\n","    \"\"\"\n","    model = ResNet3D(Bottleneck, [3, 4, 6, 3], **kwargs)\n","    return model\n","\n","def resnet101(**kwargs):\n","    \"\"\"Constructs a ResNet-101 model.\n","    \"\"\"\n","    model = ResNet3D(Bottleneck, [3, 4, 23, 3], **kwargs)\n","    return model\n","\n","def resnet152(**kwargs):\n","    \"\"\"Constructs a ResNet-101 model.\n","    \"\"\"\n","    model = ResNet3D(Bottleneck, [3, 8, 36, 3], **kwargs)\n","    return model\n","\n","def resnet200(**kwargs):\n","    \"\"\"Constructs a ResNet-101 model.\n","    \"\"\"\n","    model = ResNet3D(Bottleneck, [3, 24, 36, 3], **kwargs)\n","    return model\n"],"execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["class TReNDSModel(nn.Module):\n","    def __init__(self):\n","        super(TReNDSModel, self).__init__()\n","        \n","        # modules = list(resnet50().children())[:-1]\n","        modules = list(resnet10().children())[:-1]\n","        self.resnet = nn.Sequential(*modules)\n","        \n","        self.m1 = nn.MaxPool3d(kernel_size=(3, 3, 3))\n","        self.f0 = nn.Flatten()\n","        self.l0 = nn.Linear(5500, 1024)\n","        # self.l0 = nn.Linear(17788, 1024) # resnet10 -> 4096, resnet50 -> 16384\n","        self.p0 = nn.PReLU()\n","        self.l1 = nn.Linear(1024, 256)\n","        self.p1 = nn.PReLU()\n","        self.l2 = nn.Linear(256, 2)\n","        self.sigmoid = nn.Sigmoid()\n","        \n","        \n","    def forward(self, inputs, fnc, loading):\n","        features = self.resnet(inputs)\n","        x = self.m1(features)\n","        flatten = self.f0(x) #shape=(batch, 16384) +(batch, 1378)) + (bathc, 26)\n","        x = torch.cat([flatten, fnc, loading], dim=1) #shape(batch, 16384+1378+26)\n","        x = self.l0(x)\n","        x = self.p0(x)\n","        x = self.l1(x)\n","        x = self.p1(x)\n","        out = self.l2(x)\n","        out = self.sigmoid(out)\n","        return out"],"execution_count":11,"outputs":[]},{"cell_type":"code","execution_count":12,"metadata":{"tags":[]},"outputs":[],"source":["# m = nn.Sigmoid()\n","# loss = nn.BCELoss()\n","# input = torch.randn(3, requires_grad=True)\n","# target = torch.empty(3).random_(2)\n","# print(input.shape, target.shape)\n","# print(input, target)\n","# output = loss(m(input), target)\n","# print(output)\n","# output.backward()\n","# print(output)"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# def count_parameters(model):\n","#     params = []\n","#     for p in model.parameters():\n","#         params.append(p.numel()) \n","#     return params\n","\n","# def count_trainable_parameters(model):\n","#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","\n","# model = TReNDSModel()\n","# model\n","\n","\n","# num_parameters=count_parameters(model)\n","# print(num_parameters)\n","# num_parameters=count_trainable_parameters(model)\n","# print(num_parameters)"],"execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Dataset"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# class MRIMapDataset(Dataset):\n","#     def __init__(self, df=None, fnc=None, loading=None, mode=\"train\"):\n","#         super(Dataset, self).__init__()\n","#         self.mode = mode\n","#         self.fnc = fnc.iloc[:, 1:-2].values\n","#         self.loading = loading.iloc[:, 1:-2].values\n","        \n","#         if mode == \"train\":\n","#             # self.labels = df[['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']].values\n","#             self.labels = df[\"is_site2\"].values\n","#             self.list_IDs = df[\"Id\"].values.astype(str)\n","#         elif mode == \"test\":\n","#             list1 = os.listdir(config.root_test_path)\n","#             self.list_IDs = sorted(list1)\n","\n","#     def __len__(self):\n","#         return len(self.list_IDs)\n","    \n","#     def __getitem__(self, idx):\n","#         if self.mode == \"train\":\n","#             scan_id = self.list_IDs[idx]        \n","#             subject_filename = config.root_train_path + '/' + scan_id + '.mat'\n","#             subject_data = h5py.File(subject_filename, 'r')['SM_feature'][()]\n","#             subject_data = np.moveaxis(subject_data, [0, 1, 2, 3], [3, 2, 1, 0])\n","#             fnc = self.fnc[idx] / 600.0\n","#             loading = self.loading[idx]\n","#             return {\n","#                 'scan_maps': torch.tensor(subject_data, dtype=torch.float),\n","#                 'fnc': torch.tensor(fnc, dtype=torch.float),\n","#                 'loading': torch.tensor(loading, dtype=torch.float),\n","#                 'targets': torch.tensor(self.labels[idx, ], dtype=torch.int)\n","#             }\n","#         elif self.mode == \"test\":\n","#             scan_id = self.list_IDs[idx]        \n","#             subject_filename = config.root_test_path + '/' + scan_id\n","#             subject_data = h5py.File(subject_filename, 'r')['SM_feature'][()]\n","#             subject_data = np.moveaxis(subject_data, [0, 1, 2, 3], [3, 2, 1, 0])\n","#             fnc = self.fnc[idx].values / 600.0\n","#             loading = self.loading[idx].values\n","#             return {\n","#                 'scan_maps': torch.tensor(subject_data, dtype=torch.float),\n","#                 'fnc': torch.tensor(fnc, dtype=torch.float),\n","#                 'loading': torch.tensor(loading, dtype=torch.float),\n","#             }"],"execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["class MRIMapDataset(Dataset):\n","    def __init__(self, df=None, fnc=None, loading=None, mode=\"train\", fMRI_path=None):\n","        super(Dataset, self).__init__()\n","        self.mode = mode\n","        self.fnc = fnc.iloc[:, 1:-2].values\n","        self.loading = loading.iloc[:, 1:-2].values\n","        self.fMRI_path = fMRI_path.values\n","        if mode == \"train\":\n","            self.labels = df[\"is_site2\"].values\n","\n","    def __len__(self):\n","        return len(self.fMRI_path)\n","    \n","    def __getitem__(self, idx):\n","        if self.mode == \"train\":\n","            subject_filename = self.fMRI_path[idx]  \n","            subject_data = h5py.File(subject_filename, 'r')['SM_feature'][()]\n","            subject_data = np.moveaxis(subject_data, [0, 1, 2, 3], [3, 2, 1, 0])\n","            fnc = self.fnc[idx] / 600.0\n","            loading = self.loading[idx]\n","            return {\n","                'scan_maps': torch.tensor(subject_data, dtype=torch.float),\n","                'fnc': torch.tensor(fnc, dtype=torch.float),\n","                'loading': torch.tensor(loading, dtype=torch.float),\n","                'targets': torch.tensor(self.labels[idx, ], dtype=torch.int)\n","            }\n","        elif self.mode == \"test\":\n","            # scan_id = self.list_IDs[idx]        \n","            subject_filename = self.fMRI_path[idx]  # config.root_test_path + '/' + scan_id\n","            subject_data = h5py.File(subject_filename, 'r')['SM_feature'][()]\n","            subject_data = np.moveaxis(subject_data, [0, 1, 2, 3], [3, 2, 1, 0])\n","            fnc = self.fnc[idx].values / 600.0\n","            loading = self.loading[idx].values\n","            return {\n","                'scan_maps': torch.tensor(subject_data, dtype=torch.float),\n","                'fnc': torch.tensor(fnc, dtype=torch.float),\n","                'loading': torch.tensor(loading, dtype=torch.float),\n","            }"]},{"metadata":{},"cell_type":"markdown","source":["## Early Stopping"]},{"metadata":{"trusted":true},"cell_type":"code","source":["class EarlyStopping:\n","    def __init__(self, patience=5, checkpoint_path='checkpoint.pth', device=\"cpu\"):\n","        self.patience = patience\n","        self.checkpoint_path = checkpoint_path\n","        self.counter = 0\n","        self.best_score = None\n","        self.device = device\n","\n","    def load_best_weights(self, model):\n","        model.load_state_dict(torch.load(self.checkpoint_path, map_location=self.device))\n","\n","    def __call__(self, score, model, mode=\"min\"):\n","        # cpuでも使用できるようにするためにパラメータを一度cpuに変換してから保存し、再度deviceに直す\n","        if mode == \"max\":\n","            if self.best_score is None or (score > self.best_score):\n","                torch.save(model.to('cpu').state_dict(), self.checkpoint_path)\n","                model.to(self.device)\n","                self.best_score, self.counter = score, 0\n","                return 1\n","            else:\n","                self.counter += 1\n","                if self.counter >= self.patience:\n","                    return 2\n","        elif mode == \"min\":\n","            if self.best_score is None or (score < self.best_score):\n","                torch.save(model.to('cpu').state_dict(), self.checkpoint_path)\n","                model.to(self.device)\n","                self.best_score, self.counter = score, 0\n","                return 1\n","            else:\n","                self.counter += 1\n","                if self.counter >= self.patience:\n","                    return 2\n","        return 0\n"],"execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# GPU Fitter"]},{"metadata":{"trusted":true},"cell_type":"code","source":["class GPUFitter:\n","    def __init__(self, model, fold, device, config, save_model_path=\"checkpoint.pth\", log_path=\"log.csv\"):\n","        self.model = model\n","        self.device = device\n","        self.log_path = log_path[:-4] +f\"_fold{fold}_No{file_No}.csv\"\n","        self.save_model_path = save_model_path[:-4] +f\"_fold{fold}_No{file_No}.pth\"\n","        \n","        self.epoch = 0\n","        self.fold = fold\n","        self.config = config\n","                \n","        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.learning_rate)\n","        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","            self.optimizer, \n","            mode='min', \n","            patience=3, \n","            factor=0.3, \n","            verbose=True\n","        )\n","        self.early_stopping = EarlyStopping(patience=10, device=device, checkpoint_path=self.save_model_path)\n","        # logits = F.softmax(torch.rand(10,no_of_classes)).float()#.view(-1)\n","        # labels = torch.empty(10).random_(2).to(torch.int64)\n","        self.no_of_classes = 2\n","        self.beta = 0.9999\n","        self.gamma = 2.0\n","        self.samples_per_cls = [1176 * 4, 102 * 4]\n","        self.samples_per_cls_valid = [1176 * 4, 102 * 1]\n","        self.loss_type = \"focal\"\n","        # cb_loss = CB_loss(labels, logits, samples_per_cls, no_of_classes,loss_type, beta, gamma)\n","        self.criterion = CB_loss  # TReNDSLoss(self.device)\n","                \n","        self.log(f'Fitter prepared for fold {self.fold}. Device is {self.device} target:{config.target[target_id]}')\n","        self.columns = [\"loss\", \"score\", \"val_loss\", \"val_score\", \"lr\"]\n","        self.log_df = pd.DataFrame(columns=self.columns)\n","        \n","    def fit(self, train_loader, valid_loader):\n","        for e in range(self.config.epochs):\n","            lr = self.optimizer.param_groups[0]['lr']\n","            if self.config.verbose:\n","                timestamp = datetime.utcnow().isoformat()\n","                self.log(f'\\n{timestamp}\\nLR:{lr:.5f}')\n","            \n","            t = time.time()\n","            loss, score = self.train_one_epoch(train_loader)\n","            val_loss, val_score = self.validation_one_epoch(valid_loader)\n","            print(f'Epoch: {self.epoch}, loss: {loss.avg:.5f}, score: {score:.5f},'\\\n","                  f'val_loss: {val_loss.avg:.5f}, val_score: {val_score:.5f},'\\\n","                  f'time:{(time.time() - t):.5f}, lr{lr:.7f}' )\n","            res = self.early_stopping(val_score, self.model, mode=\"max\")\n","            self.scheduler.step(val_loss.avg)\n","            tmp = pd.DataFrame([[loss.avg, score, val_loss.avg, val_score, lr]], columns=self.columns)\n","            self.log_df = pd.concat([self.log_df, tmp], axis=0)\n","            self.log_df.to_csv(self.log_path, index=False)\n","            if res == 2:\n","                print(\"Early Stopping\")\n","                print(self.log_path)\n","                print(self.save_model_path)\n","                break\n","            self.epoch += 1\n","    \n","    def train_one_epoch(self, train_loader):\n","        self.model.train()\n","        losses = AverageMeter()\n","        t = time.time()\n","        _targets = []\n","        _outputs = []\n","        for step, data in enumerate(train_loader):\n","            scan_maps = data['scan_maps']\n","            fnc = data['fnc']\n","            loading =data['loading']\n","            targets = data['targets']\n","            \n","            scan_maps = scan_maps.to(self.device, dtype=torch.float)\n","            fnc = fnc.to(self.device, dtype=torch.float)\n","            loading = loading.to(self.device, dtype=torch.float)\n","            targets = targets.to(self.device, dtype=torch.int64)\n","            self.optimizer.zero_grad()\n","            \n","            outputs = self.model(scan_maps, fnc, loading)\n","            loss = self.criterion(targets, outputs, self.samples_per_cls, self.no_of_classes, self.loss_type, self.beta, self.gamma)\n","            \n","            batch_size = scan_maps.size(0)\n","            losses.update(loss.detach().item(), batch_size)\n","            \n","            targets = targets.detach().cpu().numpy()\n","            outputs = outputs.detach().cpu().numpy()\n","            _targets += list(targets)\n","            _outputs += list(outputs)\n","            loss.backward()\n","            self.optimizer.step()\n","            if self.config.verbose:\n","                if step % self.config.verbose_step == 0:\n","                    self.log(\n","                        f'Train Step {step}, ' + \\\n","                        f'fold {self.fold}, ' + \\\n","                        f'loss: {losses.avg:.5f}, ' + \\\n","                        f'time: {(time.time() - t):.5f}'\n","                    )\n","        scores = f1_score(_targets, _outputs) \n","        return losses, scores\n","    \n","    def validation_one_epoch(self, validation_loader):\n","        self.model.eval()\n","        \n","        losses = AverageMeter()\n","        t = time.time()\n","        _targets = []\n","        _outputs = []\n","        with torch.no_grad():\n","            for step, data in enumerate(validation_loader):\n","                scan_maps = data['scan_maps']\n","                fnc = data['fnc']\n","                loading =data['loading']\n","                targets = data['targets']\n","\n","                scan_maps = scan_maps.to(self.device, dtype=torch.float)\n","                fnc = fnc.to(self.device, dtype=torch.float)\n","                loading = loading.to(self.device, dtype=torch.float)\n","                targets = targets.to(self.device, dtype=torch.int64)\n","                outputs = self.model(scan_maps, fnc, loading)\n","\n","                loss = self.criterion(targets, outputs, self.samples_per_cls_valid, self.no_of_classes, self.loss_type, self.beta, self.gamma)\n","                batch_size = scan_maps.size(0)\n","                losses.update(loss.detach().item(), batch_size)\n","                \n","                targets = targets.detach().cpu().numpy()\n","                outputs = outputs.detach().cpu().numpy()\n","                _targets += list(targets)\n","                _outputs += list(outputs)\n","                if self.config.verbose:\n","                    if step % self.config.verbose_step == 0:\n","                        self.log(\n","                        f'Validation Step {step}, ' + \\\n","                        f'fold {self.fold}, ' + \\\n","                        f'loss: {losses.avg:.5f}, ' + \\\n","                        # f'competition metric: {scores.avg:.5f}, ' + \\\n","                        f'time: {(time.time() - t):.5f}'\n","                        )\n","        scores = f1_score(_targets, _outputs)\n","        return losses, scores\n","    \n","    def log(self, message):\n","        if self.config.verbose:\n","            print(message)"],"execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Loading data"]},{"metadata":{},"cell_type":"markdown","source":["## num_workerについて\n","\n","0だとメインプロセスのみがバッチをロードして、1以上だとサブプロセスが生えて代わりにロードしてくれるらしい  \n","これを1以上にすると、Pythonのコードを実行してるメインプロセスとは別のワーカープロセスがメインプロセスと並列的にデータのロードを行ってメモリにキューして行ってくれるので、メインプロセスは、データのロード以外の仕事に集中できる。  \n","ただし、ワーカープロセス数は増やせばいいってもんじゃなくて、メインプロセスの他の処理の忙しさとか、CPUコア数とかバッチサイズとかにも複雑に依存するので、実測値がデフォルトよりもよくなるかはわからん。  \n","ワーカープロセスが過多だと、メインプロセスが次のバッチを必要とするまでにメモリが詰まったり、その分CPUが占領され流とか、メモリが足りなくなるとか  \n","\n","#### 例\n","すべてのデータを使うと (num_worker=8, batch_size=16)のときはメモリエラー  \n","すべてのデータを使うと (num_worker=4, batch_size=16)のときはうまくいく  \n","https://deeplizard.com/learn/video/kWVgvsejXsE これがnum_workerについて分かりやすい\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"tags":[]},"outputs":[],"source":["# met = []\n","# a = np.array([1,2,3])\n","# b = np.array([2,3,4])\n","# met += list(a >= 2)\n","# print(met)\n","# met += list(b >= 2)\n","# print(met)\n","# target = [0, 1, 1, 0, 1, 1]\n","# s = f1_score(target, met)\n","# print(s)\n"]},{"metadata":{"trusted":true},"cell_type":"code","source":["train_df = pd.read_csv(\"../input/trends-assessment-prediction/train_scores.csv\")\n","drop_cols = [\"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]\n","train_df.drop(drop_cols, axis=1, inplace=True)\n","train_df[\"is_train\"] = True\n","# train_df[\"kfold\"] = df[\"kfold\"].astype(int)\n","\n","fnc = pd.read_csv(\"../input/trends-assessment-prediction/fnc.csv\")\n","fnc.fillna(fnc.mean(), inplace=True)\n","fnc = fnc.merge(train_df, on=\"Id\", how=\"left\")\n","test_fnc = fnc[fnc[\"is_train\"] != True].copy().reset_index(drop=True)\n","fnc = fnc[fnc[\"is_train\"] == True].copy().reset_index(drop=True)\n","\n","loading = pd.read_csv(\"../input/trends-assessment-prediction/loading.csv\")\n","loading.fillna(loading.mean(), inplace=True)\n","loading = loading.merge(train_df, on=\"Id\", how=\"left\")\n","test_loading = loading[loading[\"is_train\"] != True].copy().reset_index(drop=True)\n","loading = loading[loading[\"is_train\"] == True].copy().reset_index(drop=True)\n","kf = KFold(n_splits=config.num_folds, shuffle=True, random_state=config.seed)\n","for fold, (trn_, val_) in enumerate(kf.split(train_df)):\n","    loading.loc[val_, 'kfold'] = fold\n","    fnc.loc[val_, 'kfold'] = fold\n","loading = loading[loading[\"kfold\"] == config.fold]\n","fnc = fnc[fnc[\"kfold\"] == config.fold]"],"execution_count":32,"outputs":[]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["# loading.loc[:, 'kfold'] = -1\n","# fnc.loc[:, 'kfold'] = -1\n"]},{"cell_type":"code","execution_count":34,"metadata":{"tags":[]},"outputs":[],"source":["# devide test data site2 and unknow\n","site2_id = pd.read_csv(\"../input/trends-assessment-prediction/reveal_ID_site2.csv\")\n","site2_id['is_site2'] = 1\n","loading[\"is_site2\"] = 0\n","fnc[\"is_site2\"] = 0\n","test_loading = pd.merge(test_loading, site2_id, how=\"left\")\n","test_loading.loc[test_loading[\"is_site2\"] != 1, [\"is_site2\"]] = \"unknow\"\n","test_fnc = pd.merge(test_fnc, site2_id, how=\"left\")\n","test_fnc.loc[test_fnc[\"is_site2\"] != 1, [\"is_site2\"]] = \"unknow\"\n","\n","adversal_loading_df = pd.concat([loading, test_loading[test_loading[\"is_site2\"] == 1]], axis=0).reset_index(drop=True).drop(\"is_train\", axis=1)\n","adversal_fnc_df = pd.concat([fnc, test_fnc[test_fnc[\"is_site2\"] == 1]], axis=0).reset_index(drop=True).drop(\"is_train\", axis=1)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["adversal_loading_df['kfold'] = -1\n","adversal_fnc_df['kfold'] = -1\n","\n","kf = StratifiedKFold(n_splits=config.num_folds, shuffle=True, random_state=config.seed)\n","for fold, (trn_, val_) in enumerate(kf.split(adversal_loading_df[adversal_loading_df.columns[1:-1]], adversal_loading_df[\"is_site2\"].astype(int))):\n","    adversal_loading_df.loc[val_, 'kfold'] = fold\n","    adversal_fnc_df.loc[val_, 'kfold'] = fold\n"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"Id          1686\nis_site2       2\nkfold          5\npath        1686\ndtype: int64"},"metadata":{},"execution_count":36}],"source":["adversal_target_df = adversal_loading_df[[\"Id\", \"is_site2\", \"kfold\"]]\n","adversal_target_df.loc[:, 'path'] = -1\n","id_names = adversal_target_df.loc[adversal_target_df[\"is_site2\"]==0, [\"Id\"]].values.astype(str)\n","adversal_target_df.loc[adversal_target_df['is_site2']==0, ['path']] = [f\"{config.root_train_path}/{id_name}.mat\" for id_name in list(id_names.squeeze())]\n","id_names = adversal_target_df.loc[adversal_target_df[\"is_site2\"]==1, [\"Id\"]].values.astype(str)\n","adversal_target_df.loc[adversal_target_df['is_site2']==1, ['path']] = [f\"{config.root_test_path}/{id_name}.mat\" for id_name in list(id_names.squeeze())]\n","adversal_target_df.nunique()"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"      Id  SCN(53)_vs_SCN(69)  SCN(98)_vs_SCN(69)  SCN(99)_vs_SCN(69)  \\\n0  10002            0.151696           -0.024819            0.217504   \n1  10008            0.153818            0.218085            0.488672   \n2  10025            0.576461            0.366273            0.742747   \n3  10040            0.143433           -0.013474            0.370541   \n4  10044            0.433872            0.044973            0.540667   \n5  10064            0.058596            0.113231            0.416568   \n\n   SCN(45)_vs_SCN(69)  ADN(21)_vs_SCN(69)  ADN(56)_vs_SCN(69)  \\\n0            0.418072           -0.227234           -0.064052   \n1            0.432211           -0.259662           -0.081578   \n2            0.810723           -0.717543           -0.525704   \n3            0.325814           -0.050111           -0.123152   \n4            0.676584           -0.291683           -0.134273   \n5            0.452720           -0.242331           -0.222769   \n\n   SMN(3)_vs_SCN(69)  SMN(9)_vs_SCN(69)  SMN(2)_vs_SCN(69)  ...  \\\n0          -0.143832          -0.118116          -0.054825  ...   \n1          -0.058116          -0.188684          -0.070860  ...   \n2          -0.599260          -0.416625          -0.472749  ...   \n3           0.145026          -0.104776          -0.096995  ...   \n4          -0.510567          -0.557277          -0.494294  ...   \n5          -0.163339           0.107218           0.005950  ...   \n\n   CBN(4)_vs_DMN(94)  CBN(7)_vs_DMN(94)  CBN(18)_vs_CBN(13)  \\\n0           0.143014          -0.189962            0.498373   \n1          -0.386757           0.020546            0.518383   \n2          -0.058247           0.293138            0.760355   \n3          -0.089259           0.206624            0.130118   \n4          -0.005523           0.091255            0.598084   \n5          -0.024074           0.053296            0.580231   \n\n   CBN(4)_vs_CBN(13)  CBN(7)_vs_CBN(13)  CBN(4)_vs_CBN(18)  CBN(7)_vs_CBN(18)  \\\n0           0.444231           0.592438           0.028649           0.705524   \n1           0.408071           0.465851           0.112785           0.574596   \n2           0.687045           0.644372           0.455734           0.828047   \n3           0.227485           0.209705           0.099346           0.678393   \n4           0.531972           0.649238           0.274109           0.779381   \n5           0.626988           0.468206           0.421158           0.748095   \n\n   CBN(7)_vs_CBN(4)  kfold  is_site2  \n0          0.248327      0         0  \n1          0.178531      0         0  \n2          0.412260      3         0  \n3          0.162523      4         0  \n4          0.438291      4         0  \n5          0.531254      3         0  \n\n[6 rows x 1381 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>SCN(53)_vs_SCN(69)</th>\n      <th>SCN(98)_vs_SCN(69)</th>\n      <th>SCN(99)_vs_SCN(69)</th>\n      <th>SCN(45)_vs_SCN(69)</th>\n      <th>ADN(21)_vs_SCN(69)</th>\n      <th>ADN(56)_vs_SCN(69)</th>\n      <th>SMN(3)_vs_SCN(69)</th>\n      <th>SMN(9)_vs_SCN(69)</th>\n      <th>SMN(2)_vs_SCN(69)</th>\n      <th>...</th>\n      <th>CBN(4)_vs_DMN(94)</th>\n      <th>CBN(7)_vs_DMN(94)</th>\n      <th>CBN(18)_vs_CBN(13)</th>\n      <th>CBN(4)_vs_CBN(13)</th>\n      <th>CBN(7)_vs_CBN(13)</th>\n      <th>CBN(4)_vs_CBN(18)</th>\n      <th>CBN(7)_vs_CBN(18)</th>\n      <th>CBN(7)_vs_CBN(4)</th>\n      <th>kfold</th>\n      <th>is_site2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10002</td>\n      <td>0.151696</td>\n      <td>-0.024819</td>\n      <td>0.217504</td>\n      <td>0.418072</td>\n      <td>-0.227234</td>\n      <td>-0.064052</td>\n      <td>-0.143832</td>\n      <td>-0.118116</td>\n      <td>-0.054825</td>\n      <td>...</td>\n      <td>0.143014</td>\n      <td>-0.189962</td>\n      <td>0.498373</td>\n      <td>0.444231</td>\n      <td>0.592438</td>\n      <td>0.028649</td>\n      <td>0.705524</td>\n      <td>0.248327</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10008</td>\n      <td>0.153818</td>\n      <td>0.218085</td>\n      <td>0.488672</td>\n      <td>0.432211</td>\n      <td>-0.259662</td>\n      <td>-0.081578</td>\n      <td>-0.058116</td>\n      <td>-0.188684</td>\n      <td>-0.070860</td>\n      <td>...</td>\n      <td>-0.386757</td>\n      <td>0.020546</td>\n      <td>0.518383</td>\n      <td>0.408071</td>\n      <td>0.465851</td>\n      <td>0.112785</td>\n      <td>0.574596</td>\n      <td>0.178531</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10025</td>\n      <td>0.576461</td>\n      <td>0.366273</td>\n      <td>0.742747</td>\n      <td>0.810723</td>\n      <td>-0.717543</td>\n      <td>-0.525704</td>\n      <td>-0.599260</td>\n      <td>-0.416625</td>\n      <td>-0.472749</td>\n      <td>...</td>\n      <td>-0.058247</td>\n      <td>0.293138</td>\n      <td>0.760355</td>\n      <td>0.687045</td>\n      <td>0.644372</td>\n      <td>0.455734</td>\n      <td>0.828047</td>\n      <td>0.412260</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10040</td>\n      <td>0.143433</td>\n      <td>-0.013474</td>\n      <td>0.370541</td>\n      <td>0.325814</td>\n      <td>-0.050111</td>\n      <td>-0.123152</td>\n      <td>0.145026</td>\n      <td>-0.104776</td>\n      <td>-0.096995</td>\n      <td>...</td>\n      <td>-0.089259</td>\n      <td>0.206624</td>\n      <td>0.130118</td>\n      <td>0.227485</td>\n      <td>0.209705</td>\n      <td>0.099346</td>\n      <td>0.678393</td>\n      <td>0.162523</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10044</td>\n      <td>0.433872</td>\n      <td>0.044973</td>\n      <td>0.540667</td>\n      <td>0.676584</td>\n      <td>-0.291683</td>\n      <td>-0.134273</td>\n      <td>-0.510567</td>\n      <td>-0.557277</td>\n      <td>-0.494294</td>\n      <td>...</td>\n      <td>-0.005523</td>\n      <td>0.091255</td>\n      <td>0.598084</td>\n      <td>0.531972</td>\n      <td>0.649238</td>\n      <td>0.274109</td>\n      <td>0.779381</td>\n      <td>0.438291</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>10064</td>\n      <td>0.058596</td>\n      <td>0.113231</td>\n      <td>0.416568</td>\n      <td>0.452720</td>\n      <td>-0.242331</td>\n      <td>-0.222769</td>\n      <td>-0.163339</td>\n      <td>0.107218</td>\n      <td>0.005950</td>\n      <td>...</td>\n      <td>-0.024074</td>\n      <td>0.053296</td>\n      <td>0.580231</td>\n      <td>0.626988</td>\n      <td>0.468206</td>\n      <td>0.421158</td>\n      <td>0.748095</td>\n      <td>0.531254</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>6 rows × 1381 columns</p>\n</div>"},"metadata":{},"execution_count":38}],"source":["fMRI_train_path = \n","fMRI_test_path = "]},{"metadata":{},"cell_type":"markdown","source":["# Running on multiple folds"]},{"metadata":{"trusted":true,"tags":[]},"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","save_model_path = \"adversal_resnet10.pth\"\n","log_path = \"log_adversal_resnet10.csv\"\n","print(device)\n","print(save_model_path)\n","print(log_path)"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":"cuda\nadversal_resnet10.pth\nlog_adversal_resnet10.csv\n"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["def run(fold):\n","    \n","    model = TReNDSModel()\n","    model.to(device)\n","    \n","    adversal_train_loading_df = adversal_loading_df[adversal_loading_df['kfold'] != fold].reset_index(drop=True)\n","    adversal_valid_loading_df = adversal_loading_df[adversal_loading_df['kfold'] == fold].reset_index(drop=True)\n","    adversal_train_fnc_df = adversal_fnc_df[adversal_fnc_df['kfold'] != fold].reset_index(drop=True)\n","    adversal_valid_fnc_df = adversal_fnc_df[adversal_fnc_df['kfold'] == fold].reset_index(drop=True)\n","    adversal_target_train_df = adversal_target_df[adversal_target_df['kfold'] != fold].reset_index(drop=True)\n","    adversal_target_valid_df = adversal_target_df[adversal_target_df['kfold'] != fold].reset_index(drop=True)\n","    \n","    train_dataset = MRIMapDataset(df=adversal_target_train_df, fnc=adversal_train_fnc_df, loading=adversal_train_loading_df, fMRI_path= mode=\"train\")\n","    valid_dataset = MRIMapDataset(df=adversal_target_valid_df, fnc=adversal_valid_fnc_df, loading=adversal_valid_loading_df, mode=\"train\")\n","    \n","    train_data_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=config.batch_size,\n","        num_workers=config.num_workers,\n","        shuffle=True\n","    )\n","    \n","    valid_data_loader = torch.utils.data.DataLoader(\n","        valid_dataset,\n","        batch_size=config.batch_size,\n","        num_workers=config.num_workers,\n","        shuffle=False\n","    )\n","    \n","    fitter = GPUFitter(model, fold, device, config, save_model_path=save_model_path, log_path=log_path)\n","    fitter.fit(train_data_loader, valid_data_loader)\n","    print('over')\n","    return fitter"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["adversal_train_loading_df = adversal_loading_df[adversal_loading_df['kfold'] != fold].reset_index(drop=True)\n","adversal_valid_loading_df = adversal_loading_df[adversal_loading_df['kfold'] == fold].reset_index(drop=True)\n","adversal_train_fnc_df = adversal_fnc_df[adversal_fnc_df['kfold'] != fold].reset_index(drop=True)\n","adversal_valid_fnc_df = adversal_fnc_df[adversal_fnc_df['kfold'] == fold].reset_index(drop=True)\n","adversal_target_train_df = adversal_target_df[adversal_target_df['kfold'] != fold].reset_index(drop=True)\n","adversal_target_valid_df = adversal_target_df[adversal_target_df['kfold'] == fold].reset_index(drop=True)\n","\n","train_dataset = MRIMapDataset(df=adversal_target_train_df, fnc=adversal_train_fnc_df, loading=adversal_train_loading_df, fMRI_path=adversal_target_train_df['path'], mode=\"train\")\n","valid_dataset = MRIMapDataset(df=adversal_target_valid_df, fnc=adversal_valid_fnc_df, loading=adversal_valid_loading_df, fMRI_path=adversal_target_valid_df['path'], mode=\"train\")\n","train_data_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=config.batch_size,\n","        num_workers=config.num_workers,\n","        shuffle=True\n","    )\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"         Id is_site2  kfold                                               path\n0     10004        0      4  ../input/trends-assessment-prediction/fMRI_tra...\n1     10005        0      4  ../input/trends-assessment-prediction/fMRI_tra...\n2     10026        0      4  ../input/trends-assessment-prediction/fMRI_tra...\n3     10031        0      4  ../input/trends-assessment-prediction/fMRI_tra...\n4     10032        0      4  ../input/trends-assessment-prediction/fMRI_tra...\n...     ...      ...    ...                                                ...\n1272  21090        1      4  ../input/trends-assessment-prediction/fMRI_tes...\n1273  21206        1      4  ../input/trends-assessment-prediction/fMRI_tes...\n1274  21296        1      4  ../input/trends-assessment-prediction/fMRI_tes...\n1275  21409        1      4  ../input/trends-assessment-prediction/fMRI_tes...\n1276  21502        1      4  ../input/trends-assessment-prediction/fMRI_tes...\n\n[1277 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>is_site2</th>\n      <th>kfold</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10004</td>\n      <td>0</td>\n      <td>4</td>\n      <td>../input/trends-assessment-prediction/fMRI_tra...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10005</td>\n      <td>0</td>\n      <td>4</td>\n      <td>../input/trends-assessment-prediction/fMRI_tra...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10026</td>\n      <td>0</td>\n      <td>4</td>\n      <td>../input/trends-assessment-prediction/fMRI_tra...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10031</td>\n      <td>0</td>\n      <td>4</td>\n      <td>../input/trends-assessment-prediction/fMRI_tra...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10032</td>\n      <td>0</td>\n      <td>4</td>\n      <td>../input/trends-assessment-prediction/fMRI_tra...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1272</th>\n      <td>21090</td>\n      <td>1</td>\n      <td>4</td>\n      <td>../input/trends-assessment-prediction/fMRI_tes...</td>\n    </tr>\n    <tr>\n      <th>1273</th>\n      <td>21206</td>\n      <td>1</td>\n      <td>4</td>\n      <td>../input/trends-assessment-prediction/fMRI_tes...</td>\n    </tr>\n    <tr>\n      <th>1274</th>\n      <td>21296</td>\n      <td>1</td>\n      <td>4</td>\n      <td>../input/trends-assessment-prediction/fMRI_tes...</td>\n    </tr>\n    <tr>\n      <th>1275</th>\n      <td>21409</td>\n      <td>1</td>\n      <td>4</td>\n      <td>../input/trends-assessment-prediction/fMRI_tes...</td>\n    </tr>\n    <tr>\n      <th>1276</th>\n      <td>21502</td>\n      <td>1</td>\n      <td>4</td>\n      <td>../input/trends-assessment-prediction/fMRI_tes...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1277 rows × 4 columns</p>\n</div>"},"metadata":{},"execution_count":28}],"source":["# valid_dataset.__len__()\n","# adversal_target_train_df #5110 rows × 4 columns\n","adversal_target_valid_df"]},{"metadata":{"trusted":true},"cell_type":"code","source":["fitter = run(config.fold)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["log_df = pd.read_csv(fitter.log_path)\n","# log_df = pd.read_csv(\"log0.csv\")\n","# log_df = pd.read_csv('../input/trend3dcnn/log0.csv')\n","\n","log_df"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["ptcture_path = log_path[:-4] +f\"_fold{config.fold}_No{file_No}.png\"\n","plt.figure(figsize=(15,5))\n","plt.title(\"loss\")\n","plt.subplot(1,2,1)\n","log_df.loss.plot()\n","log_df.val_loss.plot()\n","plt.subplot(1,2,2)\n","plt.title(\"score\")\n","log_df.score.plot()\n","log_df.val_score.plot()\n","plt.savefig(f\"pictures/{ptcture_path}\")"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# train_df = df[df['kfold'] != fold].reset_index(drop=True)\n","# train_dataset = MRIMapDataset(df=train_df, mode=\"train\")\n","# train_dataset[0][\"scan_maps\"].shape"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Parallel(n_jobs=config.num_folds, backend=\"threading\")(delayed(run)(i) for i in range(config.num_folds))"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# valid data and test data "]},{"metadata":{"trusted":true},"cell_type":"code","source":["# resnet10でだいたい45分 num_worker=0のとき\n","adversal_valid_fnc_df = adversal_fnc_df[adversal_fnc_df['kfold'] == config.fold].reset_index(drop=True)\n","adversal_valid_loading_df = adversal_target_df[adversal_loading_df['kfold'] == config.fold].reset_index(drop=True)\n","\n","valid_dataset = MRIMapDataset(fnc=adversal_valid_fnc_df, loading=adversal_valid_loading_df, mode=\"test\")\n","valid_data_loader = torch.utils.data.DataLoader(\n","    valid_dataset,\n","    batch_size=config.batch_size,\n","    num_workers=config.num_workers,\n","    shuffle=False)\n","_test_loading = test_loading.loc[test_loading[\"is_site2\"] != 1]\n","_test_fnc = test_fnc.loc[test_fnc[\"is_site2\"] != 1]\n","test_dataset = MRIMapDataset(fnc=_test_fnc, loading=_test_loading, mode=\"test\")\n","test_dataloader = torch.utils.data.DataLoader(\n","        test_dataset,\n","        batch_size=config.batch_size,\n","        num_workers=config.num_workers,\n","        shuffle=False)\n","model = TReNDSModel()\n","model.load_state_dict(torch.load(fitter.save_model_path))#'../input/trend3dcnn/checkpoint0.pth'\n","model.to(device)\n","model.eval()\n","\n","test_preds = np.empty((0, 2))\n","valid_preds = np.empty((0, 2))\n","with torch.no_grad():\n","    for step, data in enumerate(tqdm(test_dataloader)):\n","        scan_maps = data['scan_maps']\n","        fnc = data['fnc']\n","        loading =data['loading']       \n","        scan_maps = scan_maps.to(device, dtype=torch.float)\n","        fnc = fnc.to(device, dtype=torch.float)\n","        loading = loading.to(device, dtype=torch.float)\n","        \n","        outputs = model(scan_maps, fnc, loading)\n","        batch_size = scan_maps.size(0)\n","        outputs = outputs.detach().cpu().numpy()\n","        test_preds = np.concatenate([test_preds, outputs], 0)\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","    test_df = pd.DataFrame(test_preds, columns=[\"site1\", \"site2\"]])\n","    test_df.to_csv(f\"adversal/test_fold{config.fold}_No{file_No}.csv\", index=False)\n","    for step, data in enumerate(tqdm(valid_data_loader)):\n","        scan_maps = data['scan_maps']\n","        fnc = data['fnc']\n","        loading =data['loading']       \n","        scan_maps = scan_maps.to(device, dtype=torch.float)\n","        fnc = fnc.to(device, dtype=torch.float)\n","        loading = loading.to(device, dtype=torch.float)\n","        \n","        outputs = model(scan_maps, fnc, loading)\n","        batch_size = scan_maps.size(0)\n","        outputs = outputs.detach().cpu().numpy()\n","        valid_preds = np.concatenate([valid_preds, outputs], 0)\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","    valid_df = pd.DataFrame(valid_preds, columns=[\"site1\", \"site2\"]])\n","    valid_df.to_csv(f\"adversal/valid_fold{config.fold}_No{file_No}.csv\", index=False)\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["print(outputs.shape)\n","print(test_preds)\n","print(valid_preds.shape)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# test_df = pd.DataFrame(test_preds, columns=[\"age\", \"domain1_var1\", \"domain1_var2\",\"domain2_var1\", \"domain2_var2\"])\n","# test_df = pd.DataFrame(test_preds, columns=[\"site1\", \"site2\"]])\n","# test_df.describe()\n","# test_df.to_csv(f\"test_fold{config.fold}_No{file_No}.csv\", index=False)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# list1 = os.listdir(config.root_test_path)\n","# list2 = sorted(list1)\n","# test_df[\"Id\"] = list2\n","# test_df[\"Id\"] = test_df[\"Id\"].map(lambda x: x[:-4])\n","# test_df.set_index(\"Id\", drop=True, inplace=True)\n","# test_df"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# df_long = test_df.stack().reset_index()\n","# df_long.rename(columns={'level_1': 'target', 0: 'Predicted'}, inplace=True)\n","# df_long[\"Id\"] = df_long[\"Id\"] + \"_\" + df_long[\"target\"]\n","# df_long.drop(\"target\", axis=1, inplace=True)\n","# df_long.to_csv('submission_No{file_No}.csv', index=False)"],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.8-final","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}