{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":["#target_id {0:\"age\", 1:\"domain1_var1\", 2:\"domain1_var2\", 3:\"domain2_var1\", 4:\"domain2_var2\"}\n","target_id = 0\n","file_No = 0"],"execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Importing dependencies"]},{"metadata":{"trusted":true},"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset\n","\n","import numpy as np\n","import pandas as pd\n","\n","import random\n","from tqdm.notebook import tqdm\n","import math\n","from functools import partial\n","import h5py\n","from datetime import datetime\n","import os\n","import time\n","import gc\n","from joblib import Parallel, delayed\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import KFold\n","import nilearn as nl\n","import nilearn.plotting as nlplt\n","import nibabel as nib"],"execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Config"]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":["class config:\n","    epochs = 100\n","    batch_size = 16\n","    test_batch_size = 16\n","    learning_rate = 1e-3\n","    fMRI_mask_path = '../input/trends-assessment-prediction/fMRI_mask.nii'\n","    root_train_path = '../input/trends-assessment-prediction/fMRI_train'\n","    root_test_path = '../input/trends-assessment-prediction/fMRI_test'\n","    num_folds = 5\n","    seed = 2020\n","    verbose = True\n","    verbose_step = 1\n","    num_workers = 4\n","    test_num_workers = 4\n","    target = [\"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]\n","    weight = [0.3, 0.175, 0.175, 0.175, 0.175]\n","    # cross validationをするときはここでfoldを変更する\n","    fold = 1"],"execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["print(config.target[target_id], config.weight[target_id])"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":"age 0.3\n"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","seed_everything(config.seed)"],"execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Metrics"]},{"metadata":{"trusted":true},"cell_type":"code","source":["def weighted_metric(y_true, y_pred):\n","    # weight = np.array([0.3, 0.175, 0.175, 0.175, 0.175])\n","    weight = np.array(config.weight[target_id])\n","    return np.sum(np.sum(np.abs(y_true - y_pred), axis=0) / np.sum(y_pred, axis=0) * weight)"],"execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","    \n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","    \n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Loss function"]},{"metadata":{"trusted":true},"cell_type":"code","source":["class TReNDSLoss(nn.Module):\n","    def __init__(self, device):\n","        super(TReNDSLoss, self).__init__()\n","        # self.weights = torch.tensor([.3, .175, .175, .175, .175], dtype=torch.float32, device=device)\n","        self.weights = torch.tensor(config.weight[target_id], dtype=torch.float32, device=device)\n","    def __loss(self, output, target):\n","        nom = torch.sum(torch.abs(output-target), dim=0)\n","        denom = torch.sum(target, dim=0)\n","        return torch.sum(self.weights * nom / denom)\n","\n","    def forward(self, output: torch.Tensor, target: torch.Tensor, **kwargs):\n","        return self.__loss(output, target)"],"execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Model"]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":["__all__ = [\n","    'resnet10', \n","    'resnet18', \n","    'resnet34', \n","    'resnet50', \n","    'resnet101',\n","    'resnet152', \n","    'resnet200'\n","]\n","\n","def conv3x3x3(in_planes, out_planes, stride=1, dilation=1):\n","    # 3x3x3 convolution with padding\n","    return nn.Conv3d(\n","        in_planes,\n","        out_planes,\n","        kernel_size=3,\n","        dilation=dilation,\n","        stride=stride,\n","        padding=dilation,\n","        bias=False)\n","\n","def downsample_basic_block(x, planes, stride, no_cuda=False):\n","    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n","    zero_pads = torch.Tensor(out.size(0), planes - out.size(1), out.size(2), out.size(3), out.size(4)).zero_()\n","    if not no_cuda:\n","        if isinstance(out.data, torch.cuda.FloatTensor):\n","            zero_pads = zero_pads.cuda()\n","\n","    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n","    return out\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3x3(inplanes, planes, stride=stride, dilation=dilation)\n","        self.bn1 = nn.BatchNorm3d(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3x3(planes, planes, dilation=dilation)\n","        self.bn2 = nn.BatchNorm3d(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.dilation = dilation\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm3d(planes)\n","        self.conv2 = nn.Conv3d(\n","            planes, planes, kernel_size=3, stride=stride, dilation=dilation, padding=dilation, bias=False)\n","        self.bn2 = nn.BatchNorm3d(planes)\n","        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm3d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.dilation = dilation\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","        return out\n","\n","class ResNet3D(nn.Module):\n","\n","    def __init__(self,\n","                 block,\n","                 layers,\n","                 shortcut_type='B',\n","                 num_class = 5,\n","                 no_cuda=False):\n","\n","        self.inplanes = 64\n","        self.no_cuda = no_cuda\n","        super(ResNet3D, self).__init__()\n","\n","        # 3D conv net\n","        self.conv1 = nn.Conv3d(53, 64, kernel_size=7, stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n","        # self.conv1 = nn.Conv3d(1, 64, kernel_size=7, stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n","        self.bn1 = nn.BatchNorm3d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)\n","        self.layer2 = self._make_layer(\n","            block, 64*2, layers[1], shortcut_type, stride=2)\n","        self.layer3 = self._make_layer(\n","            block, 128*2, layers[2], shortcut_type, stride=1, dilation=2)\n","        self.layer4 = self._make_layer(\n","            block, 256*2, layers[3], shortcut_type, stride=1, dilation=4)\n","\n","        self.fea_dim = 256*2 * block.expansion\n","        self.fc = nn.Sequential(nn.Linear(self.fea_dim, num_class, bias=True))\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv3d):\n","                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')\n","            elif isinstance(m, nn.BatchNorm3d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1, dilation=1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","\n","            if shortcut_type == 'A':\n","                downsample = partial(\n","                    downsample_basic_block,\n","                    planes=planes * block.expansion,\n","                    stride=stride,\n","                    no_cuda=self.no_cuda)\n","            else:\n","                downsample = nn.Sequential(\n","                    nn.Conv3d(\n","                        self.inplanes,\n","                        planes * block.expansion,\n","                        kernel_size=1,\n","                        stride=stride,\n","                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride=stride, dilation=dilation, downsample=downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, dilation=dilation))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.conv1( x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = F.adaptive_avg_pool3d(x, (1, 1, 1))\n","        emb_3d = x.view((-1, self.fea_dim))\n","        out = self.fc(emb_3d)\n","        return out\n","\n","\n","def resnet10(**kwargs):\n","    \"\"\"Constructs a ResNet-18 model.\n","    \"\"\"\n","    model = ResNet3D(BasicBlock, [1, 1, 1, 1],**kwargs)\n","    return model\n","\n","def resnet3d_10(**kwargs):\n","    \"\"\"Constructs a ResNet-18 model.\n","    \"\"\"\n","    model = ResNet3D(BasicBlock, [1, 1, 1, 1], **kwargs)\n","    return model\n","\n","def resnet18(**kwargs):\n","    \"\"\"Constructs a ResNet-18 model.\n","    \"\"\"\n","    model = ResNet3D(BasicBlock, [2, 2, 2, 2], **kwargs)\n","    return model\n","\n","def resnet34(**kwargs):\n","    \"\"\"Constructs a ResNet-34 model.\n","    \"\"\"\n","    model = ResNet3D(BasicBlock, [3, 4, 6, 3], **kwargs)\n","    return model\n","\n","def resnet50(**kwargs):\n","    \"\"\"Constructs a ResNet-50 model.\n","    \"\"\"\n","    model = ResNet3D(Bottleneck, [3, 4, 6, 3], **kwargs)\n","    return model\n","\n","def resnet101(**kwargs):\n","    \"\"\"Constructs a ResNet-101 model.\n","    \"\"\"\n","    model = ResNet3D(Bottleneck, [3, 4, 23, 3], **kwargs)\n","    return model\n","\n","def resnet152(**kwargs):\n","    \"\"\"Constructs a ResNet-101 model.\n","    \"\"\"\n","    model = ResNet3D(Bottleneck, [3, 8, 36, 3], **kwargs)\n","    return model\n","\n","def resnet200(**kwargs):\n","    \"\"\"Constructs a ResNet-101 model.\n","    \"\"\"\n","    model = ResNet3D(Bottleneck, [3, 24, 36, 3], **kwargs)\n","    return model\n"],"execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["class TReNDSModel(nn.Module):\n","    def __init__(self):\n","        super(TReNDSModel, self).__init__()\n","        \n","        # modules = list(resnet50().children())[:-1]\n","        modules = list(resnet10().children())[:-1]\n","        self.resnet = nn.Sequential(*modules)\n","        \n","        self.m1 = nn.MaxPool3d(kernel_size=(3, 3, 3))\n","        self.f0 = nn.Flatten()\n","        self.l0 = nn.Linear(5500, 1024)\n","        # self.l0 = nn.Linear(17788, 1024) # resnet10 -> 4096, resnet50 -> 16384\n","        self.p0 = nn.PReLU()\n","        self.l1 = nn.Linear(1024, 256)\n","        self.p1 = nn.PReLU()\n","        self.l2 = nn.Linear(256, 1)\n","        \n","        \n","    def forward(self, inputs, fnc, loading):\n","        features = self.resnet(inputs)\n","        x = self.m1(features)\n","        flatten = self.f0(x) #shape=(batch, 16384) +(batch, 1378)) + (bathc, 26)\n","        x = torch.cat([flatten, fnc, loading], dim=1) #shape(batch, 16384+1378+26)\n","        x = self.l0(x)\n","        x = self.p0(x)\n","        x = self.l1(x)\n","        x = self.p1(x)\n","        out = self.l2(x)\n","        return out"],"execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def count_parameters(model):\n","    params = []\n","    for p in model.parameters():\n","        params.append(p.numel()) \n","    return params\n","\n","def count_trainable_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","\n","model = TReNDSModel()\n","model\n","\n","\n","# num_parameters=count_parameters(model)\n","# print(num_parameters)\n","# num_parameters=count_trainable_parameters(model)\n","# print(num_parameters)"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":"TReNDSModel(\n  (resnet): Sequential(\n    (0): Conv3d(53, 64, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(2, 2, 2), dilation=(2, 2, 2), bias=False)\n        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(2, 2, 2), dilation=(2, 2, 2), bias=False)\n        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(4, 4, 4), dilation=(4, 4, 4), bias=False)\n        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(4, 4, 4), dilation=(4, 4, 4), bias=False)\n        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n  )\n  (m1): MaxPool3d(kernel_size=(3, 3, 3), stride=(3, 3, 3), padding=0, dilation=1, ceil_mode=False)\n  (f0): Flatten()\n  (l0): Linear(in_features=5500, out_features=1024, bias=True)\n  (p0): PReLU(num_parameters=1)\n  (l1): Linear(in_features=1024, out_features=256, bias=True)\n  (p1): PReLU(num_parameters=1)\n  (l2): Linear(in_features=256, out_features=1, bias=True)\n)"},"metadata":{},"execution_count":13}]},{"metadata":{},"cell_type":"markdown","source":["# Dataset"]},{"metadata":{"trusted":true},"cell_type":"code","source":["class MRIMapDataset(Dataset):\n","    def __init__(self, df=None, fnc=None, loading=None, mode=\"train\"):\n","        super(Dataset, self).__init__()\n","        self.mode = mode\n","        self.fnc = fnc.iloc[:, 1:-2].values\n","        self.loading = loading.iloc[:, 1:-2].values\n","        \n","        if mode == \"train\":\n","            # self.labels = df[['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']].values\n","            self.labels = df[config.target[target_id]].values\n","            self.list_IDs = df[\"Id\"].values.astype(str)\n","        elif mode == \"test\":\n","            list1 = os.listdir(config.root_test_path)\n","            self.list_IDs = sorted(list1)\n","\n","    def __len__(self):\n","        return len(self.list_IDs)\n","    \n","    def __getitem__(self, idx):\n","        if self.mode == \"train\":\n","            scan_id = self.list_IDs[idx]        \n","            subject_filename = config.root_train_path + '/' + scan_id + '.mat'\n","            subject_data = h5py.File(subject_filename, 'r')['SM_feature'][()]\n","            subject_data = np.moveaxis(subject_data, [0, 1, 2, 3], [3, 2, 1, 0])\n","            fnc = self.fnc[idx]\n","            loading = self.loading[idx]\n","            return {\n","                'scan_maps': torch.tensor(subject_data, dtype=torch.float),\n","                'fnc': torch.tensor(fnc, dtype=torch.float),\n","                'loading': torch.tensor(loading, dtype=torch.float),\n","                'targets': torch.tensor(self.labels[idx, ], dtype=torch.float)\n","            }\n","        elif self.mode == \"test\":\n","            scan_id = self.list_IDs[idx]        \n","            subject_filename = config.root_test_path + '/' + scan_id\n","            subject_data = h5py.File(subject_filename, 'r')['SM_feature'][()]\n","            subject_data = np.moveaxis(subject_data, [0, 1, 2, 3], [3, 2, 1, 0])\n","            fnc = self.fnc[idx].values\n","            loading = self.loading[idx].values\n","            return {\n","                'scan_maps': torch.tensor(subject_data, dtype=torch.float),\n","                'fnc': torch.tensor(fnc, dtype=torch.float),\n","                'loading': torch.tensor(loading, dtype=torch.float),\n","            }"],"execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## Early Stopping"]},{"metadata":{"trusted":true},"cell_type":"code","source":["class EarlyStopping:\n","    def __init__(self, patience=5, checkpoint_path='checkpoint.pth', device=\"cpu\"):\n","        self.patience = patience\n","        self.checkpoint_path = checkpoint_path\n","        self.counter = 0\n","        self.best_score = None\n","        self.device = device\n","\n","    def load_best_weights(self, model):\n","        model.load_state_dict(torch.load(self.checkpoint_path, map_location=self.device))\n","\n","    def __call__(self, score, model, mode=\"min\"):\n","        # cpuでも使用できるようにするためにパラメータを一度cpuに変換してから保存し、再度deviceに直す\n","        if mode == \"max\":\n","            if self.best_score is None or (score > self.best_score):\n","                torch.save(model.to('cpu').state_dict(), self.checkpoint_path)\n","                model.to(self.device)\n","                self.best_score, self.counter = score, 0\n","                return 1\n","            else:\n","                self.counter += 1\n","                if self.counter >= self.patience:\n","                    return 2\n","        elif mode == \"min\":\n","            if self.best_score is None or (score < self.best_score):\n","                torch.save(model.to('cpu').state_dict(), self.checkpoint_path)\n","                model.to(self.device)\n","                self.best_score, self.counter = score, 0\n","                return 1\n","            else:\n","                self.counter += 1\n","                if self.counter >= self.patience:\n","                    return 2\n","        return 0\n"],"execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# GPU Fitter"]},{"metadata":{"trusted":true},"cell_type":"code","source":["class GPUFitter:\n","    def __init__(self, model, fold, device, config, save_model_path=\"checkpoint.pth\", log_path=\"log.csv\"):\n","        self.model = model\n","        self.device = device\n","        self.log_path = log_path[:-4] +f\"_fold{fold}_No{file_No}.csv\"\n","        self.save_model_path = save_model_path[:-4] +f\"_fold{fold}_No{file_No}.pth\"\n","        \n","        self.epoch = 0\n","        self.fold = fold\n","        self.config = config\n","                \n","        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.learning_rate)\n","        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","            self.optimizer, \n","            mode='min', \n","            patience=3, \n","            factor=0.3, \n","            verbose=True\n","        )\n","        self.early_stopping = EarlyStopping(patience=10, device=device, checkpoint_path=self.save_model_path)\n","        self.criterion = TReNDSLoss(self.device)\n","                \n","        self.log(f'Fitter prepared for fold {self.fold}. Device is {self.device} target:{config.target[target_id]}')\n","        self.columns = [\"loss\", \"score\", \"val_loss\", \"val_score\", \"lr\"]\n","        self.log_df = pd.DataFrame(columns=self.columns)\n","        \n","    def fit(self, train_loader, valid_loader):\n","        for e in range(self.config.epochs):\n","            lr = self.optimizer.param_groups[0]['lr']\n","            if self.config.verbose:\n","                timestamp = datetime.utcnow().isoformat()\n","                self.log(f'\\n{timestamp}\\nLR:{lr:.5f}')\n","            \n","            t = time.time()\n","            loss, score = self.train_one_epoch(train_loader)\n","            \n","            val_loss, val_score = self.validation_one_epoch(valid_loader)\n","            print(f'Epoch: {self.epoch}, loss: {loss.avg:.5f}, score: {score.avg:.5f},'\\\n","                  f'val_loss: {val_loss.avg:.5f}, val_score: {val_score.avg:.5f},'\\\n","                  f'time:{(time.time() - t):.5f}, lr{lr:.7f}' )\n","            res = self.early_stopping(val_score.avg, self.model, mode=\"min\")\n","            self.scheduler.step(val_loss.avg)\n","            tmp = pd.DataFrame([[loss.avg, score.avg, val_loss.avg, val_score.avg, lr]], columns=self.columns)\n","            self.log_df = pd.concat([self.log_df, tmp], axis=0)\n","            self.log_df.to_csv(self.log_path, index=False)\n","            if res == 2:\n","                print(\"Early Stopping\")\n","                print(self.log_path)\n","                print(self.save_model_path)\n","                break\n","            self.epoch += 1\n","    \n","    def train_one_epoch(self, train_loader):\n","        self.model.train()\n","        losses = AverageMeter()\n","        scores = AverageMeter()\n","        t = time.time()\n","        \n","        for step, data in enumerate(train_loader):\n","            scan_maps = data['scan_maps']\n","            fnc = data['fnc']\n","            loading =data['loading']\n","            targets = data['targets']\n","            \n","            scan_maps = scan_maps.to(self.device, dtype=torch.float)\n","            fnc = fnc.to(self.device, dtype=torch.float)\n","            loading = loading.to(self.device, dtype=torch.float)\n","            targets = targets.to(self.device, dtype=torch.float)\n","            self.optimizer.zero_grad()\n","            \n","            outputs = self.model(scan_maps, fnc, loading)\n","            \n","            loss = self.criterion(outputs, targets)\n","            \n","            batch_size = scan_maps.size(0)\n","            losses.update(loss.detach().item(), batch_size)\n","            \n","            targets = targets.detach().cpu().numpy()\n","            outputs = outputs.detach().cpu().numpy()\n","            \n","            metric = weighted_metric(targets, outputs)\n","            scores.update(metric, batch_size)\n","            loss.backward()\n","            self.optimizer.step()\n","            if self.config.verbose:\n","                if step % self.config.verbose_step == 0:\n","                    self.log(\n","                        f'Train Step {step}, ' + \\\n","                        f'fold {self.fold}, ' + \\\n","                        f'loss: {losses.avg:.5f}, ' + \\\n","                        f'competition metric: {scores.avg:.5f}, ' + \\\n","                        f'time: {(time.time() - t):.5f}'\n","                    )\n","        \n","        self.model.eval()\n","        \n","        return losses, scores\n","    \n","    def validation_one_epoch(self, validation_loader):\n","        self.model.eval()\n","        \n","        losses = AverageMeter()\n","        scores = AverageMeter()\n","        t = time.time()\n","        \n","        with torch.no_grad():\n","            for step, data in enumerate(validation_loader):\n","                scan_maps = data['scan_maps']\n","                fnc = data['fnc']\n","                loading =data['loading']\n","                targets = data['targets']\n","\n","                scan_maps = scan_maps.to(self.device, dtype=torch.float)\n","                fnc = fnc.to(self.device, dtype=torch.float)\n","                loading = loading.to(self.device, dtype=torch.float)\n","                targets = targets.to(self.device, dtype=torch.float)\n","                outputs = self.model(scan_maps, fnc, loading)\n","\n","                loss = self.criterion(outputs, targets)\n","                batch_size = scan_maps.size(0)\n","                losses.update(loss.detach().item(), batch_size)\n","                \n","                targets = targets.detach().cpu().numpy()\n","                outputs = outputs.detach().cpu().numpy()\n","                metric = weighted_metric(targets, outputs)\n","                scores.update(metric, batch_size)\n","                if self.config.verbose:\n","                    if step % self.config.verbose_step == 0:\n","                        self.log(\n","                        f'Validation Step {step}, ' + \\\n","                        f'fold {self.fold}, ' + \\\n","                        f'loss: {losses.avg:.5f}, ' + \\\n","                        f'competition metric: {scores.avg:.5f}, ' + \\\n","                        f'time: {(time.time() - t):.5f}'\n","                        )\n","                \n","        return losses, scores\n","    \n","    def log(self, message):\n","        if self.config.verbose:\n","            print(message)"],"execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Loading data"]},{"metadata":{},"cell_type":"markdown","source":["## num_workerについて\n","\n","0だとメインプロセスのみがバッチをロードして、1以上だとサブプロセスが生えて代わりにロードしてくれるらしい  \n","これを1以上にすると、Pythonのコードを実行してるメインプロセスとは別のワーカープロセスがメインプロセスと並列的にデータのロードを行ってメモリにキューして行ってくれるので、メインプロセスは、データのロード以外の仕事に集中できる。  \n","ただし、ワーカープロセス数は増やせばいいってもんじゃなくて、メインプロセスの他の処理の忙しさとか、CPUコア数とかバッチサイズとかにも複雑に依存するので、実測値がデフォルトよりもよくなるかはわからん。  \n","ワーカープロセスが過多だと、メインプロセスが次のバッチを必要とするまでにメモリが詰まったり、その分CPUが占領され流とか、メモリが足りなくなるとか  \n","\n","#### 例\n","すべてのデータを使うと (num_worker=8, batch_size=16)のときはメモリエラー  \n","すべてのデータを使うと (num_worker=4, batch_size=16)のときはうまくいく  \n","https://deeplizard.com/learn/video/kWVgvsejXsE これがnum_workerについて分かりやすい\n"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# nrowsで読み込むtrainデータ数を決定している. すべて使う場合はnrow消す\n","# df = pd.read_csv('../input/trends-assessment-prediction/train_scores.csv', nrows=50)\n","\n","df = pd.read_csv('../input/trends-assessment-prediction/train_scores.csv')\n","\n","df['kfold'] = -1\n","df = df.fillna(df.mean())\n","\n","kf = KFold(n_splits=config.num_folds, shuffle=True)\n","for fold, (trn_, val_) in enumerate(kf.split(df)):\n","    df.loc[val_, 'kfold'] = fold\n","df"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":"         Id        age  domain1_var1  domain1_var2  domain2_var1  \\\n0     10001  57.436077     30.571975     62.553736     53.325130   \n1     10002  59.580851     50.969456     67.470628     60.651856   \n2     10004  71.413018     53.152498     58.012103     52.418389   \n3     10005  66.532630     51.474692     59.244132     52.108977   \n4     10007  38.617381     49.197021     65.674285     40.151376   \n...     ...        ...           ...           ...           ...   \n5872  21746  14.257265     21.358872     61.165998     51.778483   \n5873  21747  55.456978     68.169675     29.907995     55.349257   \n5874  21750  48.948756     55.114811     60.878271     38.617246   \n5875  21752  66.532630     59.844808     72.303110     55.458281   \n5876  21754  68.820928     56.594193     34.605868     49.922535   \n\n      domain2_var2  kfold  \n0        51.427998      2  \n1        58.311361      0  \n2        62.536641      2  \n3        69.993075      2  \n4        34.096421      3  \n...            ...    ...  \n5872     54.640179      1  \n5873     54.019517      1  \n5874     50.679885      3  \n5875     46.870235      3  \n5876     50.383078      0  \n\n[5877 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>age</th>\n      <th>domain1_var1</th>\n      <th>domain1_var2</th>\n      <th>domain2_var1</th>\n      <th>domain2_var2</th>\n      <th>kfold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10001</td>\n      <td>57.436077</td>\n      <td>30.571975</td>\n      <td>62.553736</td>\n      <td>53.325130</td>\n      <td>51.427998</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10002</td>\n      <td>59.580851</td>\n      <td>50.969456</td>\n      <td>67.470628</td>\n      <td>60.651856</td>\n      <td>58.311361</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10004</td>\n      <td>71.413018</td>\n      <td>53.152498</td>\n      <td>58.012103</td>\n      <td>52.418389</td>\n      <td>62.536641</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10005</td>\n      <td>66.532630</td>\n      <td>51.474692</td>\n      <td>59.244132</td>\n      <td>52.108977</td>\n      <td>69.993075</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10007</td>\n      <td>38.617381</td>\n      <td>49.197021</td>\n      <td>65.674285</td>\n      <td>40.151376</td>\n      <td>34.096421</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5872</th>\n      <td>21746</td>\n      <td>14.257265</td>\n      <td>21.358872</td>\n      <td>61.165998</td>\n      <td>51.778483</td>\n      <td>54.640179</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5873</th>\n      <td>21747</td>\n      <td>55.456978</td>\n      <td>68.169675</td>\n      <td>29.907995</td>\n      <td>55.349257</td>\n      <td>54.019517</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5874</th>\n      <td>21750</td>\n      <td>48.948756</td>\n      <td>55.114811</td>\n      <td>60.878271</td>\n      <td>38.617246</td>\n      <td>50.679885</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5875</th>\n      <td>21752</td>\n      <td>66.532630</td>\n      <td>59.844808</td>\n      <td>72.303110</td>\n      <td>55.458281</td>\n      <td>46.870235</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5876</th>\n      <td>21754</td>\n      <td>68.820928</td>\n      <td>56.594193</td>\n      <td>34.605868</td>\n      <td>49.922535</td>\n      <td>50.383078</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5877 rows × 7 columns</p>\n</div>"},"metadata":{},"execution_count":17}]},{"metadata":{"trusted":true},"cell_type":"code","source":["# df = pd.read_csv('../input/trends-assessment-prediction/train_scores.csv')\n","# list_IDs = df[\"Id\"].values.astype(str)\n","# idx = 1\n","# scan_id = list_IDs[idx]\n","# subject_filename = config.root_train_path + '/' + str(scan_id) + '.mat'\n","# subject_data = h5py.File(subject_filename, 'r')['SM_feature'][()]\n","# subject_data = np.moveaxis(subject_data, [0, 1, 2, 3], [3, 2, 1, 0])\n","# print(subject_filename)\n","# print(subject_data.shape)"],"execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["df['kfold'].value_counts()"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":"1    1176\n0    1176\n3    1175\n2    1175\n4    1175\nName: kfold, dtype: int64"},"metadata":{},"execution_count":19}]},{"metadata":{"trusted":true},"cell_type":"code","source":["train_df = pd.read_csv(\"../input/trends-assessment-prediction/train_scores.csv\")\n","drop_cols = [\"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]\n","train_df.drop(drop_cols, axis=1, inplace=True)\n","train_df[\"is_train\"] = True\n","train_df[\"kfold\"] = df[\"kfold\"].astype(int)\n","\n","fnc = pd.read_csv(\"../input/trends-assessment-prediction/fnc.csv\")\n","fnc.fillna(fnc.mean(), inplace=True)\n","fnc = fnc.merge(train_df, on=\"Id\", how=\"left\")\n","test_fnc = fnc[fnc[\"is_train\"] != True].copy()\n","fnc = fnc[fnc[\"is_train\"] == True].copy()\n","\n","loading = pd.read_csv(\"../input/trends-assessment-prediction/loading.csv\")\n","loading.fillna(loading.mean(), inplace=True)\n","loading = loading.merge(train_df, on=\"Id\", how=\"left\")\n","test_loading = loading[loading[\"is_train\"] != True].copy()\n","loading = loading[loading[\"is_train\"] == True].copy()"],"execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Running on multiple folds"]},{"metadata":{"trusted":true},"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","save_model_path = \"resnet10.pth\"\n","log_path = \"log_resnet10.csv\"\n","print(device)\n","print(save_model_path)\n","print(log_path)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def run(fold):\n","    \n","    model = TReNDSModel()\n","    model.to(device)\n","    \n","    train_df = df[df['kfold'] != fold].reset_index(drop=True)\n","    valid_df = df[df['kfold'] == fold].reset_index(drop=True)\n","    train_fnc = fnc[fnc['kfold'] != fold].reset_index(drop=True)\n","    valid_fnc = fnc[fnc['kfold'] != fold].reset_index(drop=True)\n","    train_loading = loading[loading['kfold'] != fold].reset_index(drop=True)\n","    valid_loading = loading[loading['kfold'] != fold].reset_index(drop=True)\n","    \n","    train_dataset = MRIMapDataset(df=train_df, fnc=train_fnc, loading=train_loading, mode=\"train\")\n","    valid_dataset = MRIMapDataset(df=valid_df, fnc=valid_fnc, loading=valid_loading, mode=\"train\")\n","    \n","    train_data_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=config.batch_size,\n","        num_workers=config.num_workers\n","    )\n","    \n","    valid_data_loader = torch.utils.data.DataLoader(\n","        valid_dataset,\n","        batch_size=config.batch_size,\n","        num_workers=config.num_workers\n","    )\n","    \n","    fitter = GPUFitter(model, fold, device, config, save_model_path=save_model_path, log_path=log_path)\n","    fitter.fit(train_data_loader, valid_data_loader)\n","    print('over')\n","    return fitter"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["fitter = run(config.fold)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["log_df = pd.read_csv(fitter.log_path)\n","# log_df = pd.read_csv(\"log0.csv\")\n","# log_df = pd.read_csv('../input/trend3dcnn/log0.csv')\n","\n","log_df"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["plt.figure(figsize=(15,5))\n","plt.title(\"loss\")\n","plt.subplot(1,2,1)\n","log_df.loss.plot()\n","log_df.val_loss.plot()\n","plt.subplot(1,2,2)\n","plt.title(\"score\")\n","log_df.score.plot()\n","log_df.val_score.plot()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# train_df = df[df['kfold'] != fold].reset_index(drop=True)\n","# train_dataset = MRIMapDataset(df=train_df, mode=\"train\")\n","# train_dataset[0][\"scan_maps\"].shape"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Parallel(n_jobs=config.num_folds, backend=\"threading\")(delayed(run)(i) for i in range(config.num_folds))"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# submit test data "]},{"metadata":{"trusted":true},"cell_type":"code","source":["# resnet10でだいたい45分 num_worker=0のとき\n","test_dataset = MRIMapDataset(fnc=test_fnc, loading=test_loading, mode=\"test\")\n","test_dataloader = torch.utils.data.DataLoader(\n","        test_dataset,\n","        batch_size=config.batch_size,\n","        num_workers=config.num_workers\n","    )\n","model = TReNDSModel()\n","model.load_state_dict(torch.load(fitter.save_model_path))#'../input/trend3dcnn/checkpoint0.pth'\n","model.to(device)\n","model.eval()\n","\n","test_preds = np.empty((0,5))\n","with torch.no_grad():\n","    for step, data in enumerate(tqdm(test_dataloader)):\n","        scan_maps = data['scan_maps']\n","        fnc = data['fnc']\n","        loading =data['loading']       \n","        scan_maps = scan_maps.to(device, dtype=torch.float)\n","        fnc = fnc.to(device, dtype=torch.float)\n","        loading = loading.to(device, dtype=torch.float)\n","        \n","        outputs = model(scan_maps, fnc, loading)\n","        batch_size = scan_maps.size(0)\n","        outputs = outputs.detach().cpu().numpy()\n","        test_preds = np.concatenate([test_preds, outputs], 0)\n","        torch.cuda.empty_cache()\n","        gc.collect()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["print(outputs.shape)\n","print(test_preds)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# test_df = pd.DataFrame(test_preds, columns=[\"age\", \"domain1_var1\", \"domain1_var2\",\"domain2_var1\", \"domain2_var2\"])\n","test_df = pd.DataFrame(test_preds, columns=[config.target[target_id]])\n","test_df.describe()\n","test_df.to_csv(f\"test_fold{config.fold}_No{file_No}.csv\", index=False)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["list1 = os.listdir(config.root_test_path)\n","list2 = sorted(list1)\n","test_df[\"Id\"] = list2\n","test_df[\"Id\"] = test_df[\"Id\"].map(lambda x: x[:-4])\n","test_df.set_index(\"Id\", drop=True, inplace=True)\n","test_df"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["df_long = test_df.stack().reset_index()\n","df_long.rename(columns={'level_1': 'target', 0: 'Predicted'}, inplace=True)\n","df_long[\"Id\"] = df_long[\"Id\"] + \"_\" + df_long[\"target\"]\n","df_long.drop(\"target\", axis=1, inplace=True)\n","df_long.to_csv('submission_No{file_No}.csv', index=False)"],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.8-final","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}